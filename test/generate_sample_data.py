#!/usr/bin/env python3
"""
ç”Ÿæˆç¤ºä¾‹ DeepEval æ•¸æ“š
ç”¨æ–¼æ¸¬è©¦å„€è¡¨æ¿åŠŸèƒ½
"""

import json
import random
from datetime import datetime
from typing import List, Dict, Any

def generate_sample_evaluation_data(num_cases: int = 20) -> List[Dict[str, Any]]:
    """ç”Ÿæˆç¤ºä¾‹è©•ä¼°æ•¸æ“š"""
    
    # æ³•å¾‹ç›¸é—œçš„ç¤ºä¾‹å•é¡Œ
    sample_questions = [
        "ä»€éº¼æ˜¯æ†²æ³•ç¬¬7æ¢çš„å¹³ç­‰åŸå‰‡ï¼Ÿ",
        "æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ", 
        "åˆ‘æ³•çš„ç½ªåˆ‘æ³•å®šåŸå‰‡å¦‚ä½•ç†è§£ï¼Ÿ",
        "è¡Œæ”¿æ³•ä¸­çš„æ¯”ä¾‹åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
        "æ°‘äº‹è¨´è¨Ÿä¸­çš„èˆ‰è­‰è²¬ä»»å¦‚ä½•åˆ†é…ï¼Ÿ",
        "ä»€éº¼æ˜¯ç‰©æ¬Šå’Œå‚µæ¬Šçš„å€åˆ¥ï¼Ÿ",
        "ä¾µæ¬Šè¡Œç‚ºçš„æ§‹æˆè¦ä»¶æœ‰å“ªäº›ï¼Ÿ",
        "çŠ¯ç½ªçš„æ§‹æˆè¦ä»¶åŒ…æ‹¬ä»€éº¼ï¼Ÿ",
        "æ­£ç•¶é˜²è¡›çš„æˆç«‹æ¢ä»¶æ˜¯ä»€éº¼ï¼Ÿ",
        "è¡Œæ”¿è™•åˆ†çš„å®šç¾©å’Œç‰¹å¾µï¼Ÿ",
        "åœ‹å®¶è³ å„Ÿçš„é©ç”¨ç¯„åœï¼Ÿ",
        "åŸºæœ¬äººæ¬Šçš„æ ¸å¿ƒå…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
        "æ¬ŠåŠ›åˆ†ç«‹åŸå‰‡å¦‚ä½•é«”ç¾ï¼Ÿ",
        "æ³•æ²»åœ‹å®¶çš„åŸºæœ¬è¦æ±‚ï¼Ÿ",
        "æ†²æ³•ä¿®æ­£çš„ç¨‹åºç‚ºä½•ï¼Ÿ"
    ]
    
    sample_answers = [
        "æ†²æ³•ç¬¬7æ¢è¦å®šä¸­è¯æ°‘åœ‹äººæ°‘ï¼Œç„¡åˆ†ç”·å¥³ã€å®—æ•™ã€ç¨®æ—ã€éšç´šã€é»¨æ´¾ï¼Œåœ¨æ³•å¾‹ä¸Šä¸€å¾‹å¹³ç­‰ã€‚",
        "å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯æŒ‡ç•¶äº‹äººæœ‰è‡ªç”±æ±ºå®šæ˜¯å¦ç· çµå¥‘ç´„ã€èˆ‡ä½•äººç· çµå¥‘ç´„ã€å¥‘ç´„å…§å®¹ç­‰çš„æ¬Šåˆ©ã€‚",
        "ç½ªåˆ‘æ³•å®šåŸå‰‡æ˜¯æŒ‡çŠ¯ç½ªå’Œåˆ‘ç½°å¿…é ˆç”±æ³•å¾‹æ˜æ–‡è¦å®šï¼Œä¸å¾—ä»»æ„æ“´å¼µè§£é‡‹ã€‚",
        "æ¯”ä¾‹åŸå‰‡è¦æ±‚è¡Œæ”¿è¡Œç‚ºå¿…é ˆé©ç•¶ã€å¿…è¦ä¸”ä¸éåº¦ï¼Œä»¥é”æˆè¡Œæ”¿ç›®çš„ã€‚",
        "èˆ‰è­‰è²¬ä»»åŸå‰‡ä¸Šç”±ä¸»å¼µäº‹å¯¦å­˜åœ¨çš„ç•¶äº‹äººè² æ“”ï¼Œä½†æ³•å¾‹å¦æœ‰è¦å®šè€…é™¤å¤–ã€‚"
    ]
    
    results = []
    
    for i in range(num_cases):
        # éš¨æ©Ÿé¸æ“‡å•é¡Œå’Œç­”æ¡ˆ
        question = random.choice(sample_questions)
        expected_answer = random.choice(sample_answers)
        
        # ç”Ÿæˆå¯¦éš›å›ç­”ï¼ˆç¨ä½œè®ŠåŒ–ï¼‰
        actual_answer = expected_answer
        if random.random() < 0.3:  # 30% æ©Ÿç‡ç”Ÿæˆä¸å®Œæ•´ç­”æ¡ˆ
            actual_answer = expected_answer[:len(expected_answer)//2] + "..."
        
        # ç”ŸæˆæŒ‡æ¨™åˆ†æ•¸
        base_score = random.uniform(0.4, 0.95)
        
        # ç›¸é—œæ€§åˆ†æ•¸
        answer_relevancy = max(0.1, min(0.99, base_score + random.uniform(-0.1, 0.1)))
        
        # å¿ å¯¦åº¦åˆ†æ•¸
        faithfulness = max(0.1, min(0.99, base_score + random.uniform(-0.15, 0.1)))
        
        # ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦
        contextual_precision = max(0.1, min(0.99, base_score + random.uniform(-0.1, 0.15)))
        
        # ä¸Šä¸‹æ–‡å¬å›ç‡
        contextual_recall = max(0.1, min(0.99, base_score + random.uniform(-0.1, 0.1)))
        
        # å¹»è¦ºåˆ†æ•¸ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        hallucination = max(0.01, min(0.8, 0.5 - base_score + random.uniform(-0.1, 0.2)))
        
        # åè¦‹åˆ†æ•¸ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        bias = max(0.01, min(0.7, 0.4 - base_score * 0.5 + random.uniform(-0.1, 0.1)))
        
        # è¨ˆç®—æ•´é«”åˆ†æ•¸
        positive_metrics = [answer_relevancy, faithfulness, contextual_precision, contextual_recall]
        negative_metrics = [hallucination, bias]
        
        overall_score = (
            sum(positive_metrics) / len(positive_metrics) * 0.7 +
            (1 - sum(negative_metrics) / len(negative_metrics)) * 0.3
        )
        
        # åˆ¤æ–·æ˜¯å¦é€šé
        passed = (
            answer_relevancy >= 0.7 and
            faithfulness >= 0.7 and
            contextual_precision >= 0.7 and
            contextual_recall >= 0.7 and
            hallucination <= 0.3 and
            bias <= 0.5
        )
        
        result = {
            "test_case_id": f"test_{i+1:03d}",
            "question": question,
            "actual_output": actual_answer,
            "expected_output": expected_answer,
            "retrieval_context": [
                f"ç›¸é—œæ³•æ¢å…§å®¹ç‰‡æ®µ {j+1}" for j in range(random.randint(1, 4))
            ],
            "metrics_scores": {
                "answer_relevancy": round(answer_relevancy, 3),
                "faithfulness": round(faithfulness, 3),
                "contextual_precision": round(contextual_precision, 3),
                "contextual_recall": round(contextual_recall, 3),
                "hallucination": round(hallucination, 3),
                "bias": round(bias, 3)
            },
            "overall_score": round(overall_score, 3),
            "passed": passed
        }
        
        results.append(result)
    
    return results

def generate_summary_data(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ç”Ÿæˆæ‘˜è¦æ•¸æ“š"""
    total_cases = len(results)
    passed_cases = sum(1 for r in results if r['passed'])
    pass_rate = passed_cases / total_cases * 100 if total_cases > 0 else 0
    avg_score = sum(r['overall_score'] for r in results) / total_cases if total_cases > 0 else 0
    
    return {
        'timestamp': datetime.now().isoformat(),
        'total_cases': total_cases,
        'passed_cases': passed_cases,
        'overall_pass_rate': pass_rate,
        'overall_avg_score': avg_score,
        'dataset_results': [{
            'dataset_name': 'æ³•å¾‹è€ƒè©¦æ•¸æ“šé›†',
            'total_cases': total_cases,
            'passed_cases': passed_cases,
            'pass_rate': pass_rate,
            'avg_score': avg_score,
            'results': results
        }]
    }

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ“Š ç”Ÿæˆ DeepEval ç¤ºä¾‹æ•¸æ“š")
    print("=" * 30)
    
    # ç²å–ç”¨æˆ¶è¼¸å…¥
    try:
        num_cases = int(input("è«‹è¼¸å…¥è¦ç”Ÿæˆçš„æ¸¬è©¦æ¡ˆä¾‹æ•¸é‡ [é è¨­: 20]: ") or "20")
        num_cases = max(1, min(100, num_cases))  # é™åˆ¶åœ¨ 1-100 ä¹‹é–“
    except ValueError:
        num_cases = 20
    
    print(f"ğŸ“ ç”Ÿæˆ {num_cases} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
    
    # ç”Ÿæˆæ•¸æ“š
    results = generate_sample_evaluation_data(num_cases)
    summary = generate_summary_data(results)
    
    # ä¿å­˜çµæœæ–‡ä»¶
    results_filename = f"sample_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    summary_filename = f"sample_evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    # ä¿å­˜è©³ç´°çµæœ
    with open(results_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ‘˜è¦
    with open(summary_filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… è©³ç´°çµæœå·²ä¿å­˜åˆ°: {results_filename}")
    print(f"âœ… æ‘˜è¦æ•¸æ“šå·²ä¿å­˜åˆ°: {summary_filename}")
    
    # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
    passed_count = sum(1 for r in results if r['passed'])
    pass_rate = passed_count / num_cases * 100
    avg_score = sum(r['overall_score'] for r in results) / num_cases
    
    print(f"\nğŸ“ˆ ç”Ÿæˆæ•¸æ“šçµ±è¨ˆ:")
    print(f"   ç¸½æ¡ˆä¾‹æ•¸: {num_cases}")
    print(f"   é€šéæ¡ˆä¾‹: {passed_count}")
    print(f"   é€šéç‡: {pass_rate:.1f}%")
    print(f"   å¹³å‡åˆ†æ•¸: {avg_score:.3f}")
    
    print(f"\nğŸš€ ç¾åœ¨ä½ å¯ä»¥:")
    print(f"   1. é‹è¡Œå„€è¡¨æ¿: python3 run_dashboard.py")
    print(f"   2. ç›´æ¥å•Ÿå‹•: streamlit run deepeval_dashboard.py")
    print(f"   3. åœ¨å„€è¡¨æ¿ä¸­è¼‰å…¥æ–‡ä»¶: {results_filename}")

if __name__ == "__main__":
    main()