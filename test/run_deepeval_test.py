#!/usr/bin/env python3
"""
DeepEval å®Œæ•´æ¸¬è©¦è…³æœ¬
æ¼”ç¤ºå®Œæ•´çš„ RAGFlow è©•ä¼°æµç¨‹
"""

import sys
import os
import json
from datetime import datetime

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from deepeval_integration import RAGFlowEvaluator
from deepeval_config import DeepEvalConfig

def run_comprehensive_evaluation():
    """é‹è¡Œå®Œæ•´çš„è©•ä¼°æµç¨‹"""
    print("ğŸ§ª RAGFlow DeepEval å®Œæ•´è©•ä¼°")
    print("=" * 50)
    
    # é¡¯ç¤ºé…ç½®ç‹€æ…‹
    DeepEvalConfig.print_config_status()
    print()
    
    # å‰µå»ºè©•ä¼°å™¨
    evaluator = RAGFlowEvaluator()
    
    # ç²å–æ•¸æ“šé›†åˆ—è¡¨
    print("ğŸ“š ç²å–æ•¸æ“šé›†åˆ—è¡¨...")
    datasets_result = evaluator.client.list_datasets()
    
    if not datasets_result['success']:
        print(f"âŒ ç²å–æ•¸æ“šé›†å¤±æ•—: {datasets_result['message']}")
        return False
    
    datasets = datasets_result['data']
    if not datasets:
        print("âŒ æ²’æœ‰å¯ç”¨çš„æ•¸æ“šé›†")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(datasets)} å€‹æ•¸æ“šé›†:")
    for i, dataset in enumerate(datasets, 1):
        doc_count = dataset.get('document_count', 'N/A')
        print(f"  {i}. {dataset.get('name', 'N/A')} ({doc_count} æ–‡ä»¶)")
    
    # é¸æ“‡æ•¸æ“šé›†é€²è¡Œè©•ä¼°
    results_summary = []
    
    for dataset in datasets[:2]:  # è©•ä¼°å‰å…©å€‹æ•¸æ“šé›†
        dataset_id = dataset['id']
        dataset_name = dataset['name']
        
        print(f"\nğŸ” è©•ä¼°æ•¸æ“šé›†: {dataset_name}")
        print("-" * 40)
        
        # è¨­ç½®èŠå¤©æ©Ÿå™¨äºº
        if not evaluator.setup_chatbot(dataset_id, dataset_name):
            print(f"âŒ æ•¸æ“šé›† {dataset_name} è¨­ç½®å¤±æ•—ï¼Œè·³é")
            continue
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        print("ğŸ“ ç”Ÿæˆæ¸¬è©¦å•é¡Œ...")
        test_data = evaluator.generate_test_data_from_documents(
            dataset_id, 
            DeepEvalConfig.DEFAULT_QUESTION_COUNT
        )
        
        if not test_data:
            print(f"âŒ æ•¸æ“šé›† {dataset_name} æ¸¬è©¦æ•¸æ“šç”Ÿæˆå¤±æ•—ï¼Œè·³é")
            continue
        
        print(f"âœ… ç”Ÿæˆäº† {len(test_data)} å€‹æ¸¬è©¦å•é¡Œ")
        
        # é¡¯ç¤ºéƒ¨åˆ†å•é¡Œ
        print("ğŸ“‹ æ¸¬è©¦å•é¡Œé è¦½:")
        for i, data in enumerate(test_data[:3], 1):
            print(f"  {i}. {data['question']}")
        if len(test_data) > 3:
            print(f"  ... é‚„æœ‰ {len(test_data) - 3} å€‹å•é¡Œ")
        
        # åŸ·è¡Œè©•ä¼°
        print("\nğŸ§ª é–‹å§‹è©•ä¼°...")
        results = evaluator.evaluate_test_cases(test_data)
        
        if not results:
            print(f"âŒ æ•¸æ“šé›† {dataset_name} è©•ä¼°å¤±æ•—ï¼Œè·³é")
            continue
        
        # ç”Ÿæˆå ±å‘Š
        report = evaluator.generate_report(results)
        print(report)
        
        # ä¿å­˜çµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_{dataset_name}_{timestamp}.json"
        evaluator.save_results(results, filename)
        
        # æ”¶é›†æ‘˜è¦ä¿¡æ¯
        total_cases = len(results)
        passed_cases = sum(1 for r in results if r.passed)
        avg_score = sum(r.overall_score for r in results) / total_cases
        
        results_summary.append({
            'dataset_name': dataset_name,
            'total_cases': total_cases,
            'passed_cases': passed_cases,
            'pass_rate': passed_cases / total_cases * 100,
            'avg_score': avg_score,
            'filename': filename
        })
    
    # ç”Ÿæˆç¸½é«”æ‘˜è¦
    if results_summary:
        print("\nğŸ“Š è©•ä¼°ç¸½çµ")
        print("=" * 50)
        
        for summary in results_summary:
            print(f"ğŸ“– {summary['dataset_name']}:")
            print(f"   é€šéç‡: {summary['passed_cases']}/{summary['total_cases']} ({summary['pass_rate']:.1f}%)")
            print(f"   å¹³å‡åˆ†æ•¸: {summary['avg_score']:.3f}")
            print(f"   çµæœæ–‡ä»¶: {summary['filename']}")
            print()
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        total_all = sum(s['total_cases'] for s in results_summary)
        passed_all = sum(s['passed_cases'] for s in results_summary)
        avg_all = sum(s['avg_score'] * s['total_cases'] for s in results_summary) / total_all
        
        print(f"ğŸ¯ æ•´é«”è¡¨ç¾:")
        print(f"   ç¸½æ¸¬è©¦æ¡ˆä¾‹: {total_all}")
        print(f"   ç¸½é€šéæ¡ˆä¾‹: {passed_all}")
        print(f"   æ•´é«”é€šéç‡: {passed_all/total_all*100:.1f}%")
        print(f"   æ•´é«”å¹³å‡åˆ†æ•¸: {avg_all:.3f}")
        
        # ä¿å­˜ç¸½çµ
        summary_filename = f"evaluation_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'datasets_evaluated': len(results_summary),
                'total_cases': total_all,
                'passed_cases': passed_all,
                'overall_pass_rate': passed_all/total_all*100,
                'overall_avg_score': avg_all,
                'dataset_results': results_summary
            }, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç¸½çµå·²ä¿å­˜åˆ° {summary_filename}")
    
    print("\nğŸ‰ å®Œæ•´è©•ä¼°å®Œæˆï¼")
    return True

def run_single_dataset_evaluation():
    """é‹è¡Œå–®ä¸€æ•¸æ“šé›†çš„è©³ç´°è©•ä¼°"""
    print("ğŸ¯ å–®ä¸€æ•¸æ“šé›†è©³ç´°è©•ä¼°")
    print("=" * 40)
    
    evaluator = RAGFlowEvaluator()
    
    # ç²å–æ•¸æ“šé›†
    datasets_result = evaluator.client.list_datasets()
    if not datasets_result['success'] or not datasets_result['data']:
        print("âŒ ç„¡æ³•ç²å–æ•¸æ“šé›†")
        return False
    
    datasets = datasets_result['data']
    print(f"ğŸ“š å¯ç”¨æ•¸æ“šé›†:")
    for i, dataset in enumerate(datasets, 1):
        print(f"  {i}. {dataset.get('name', 'N/A')}")
    
    # é¸æ“‡æ•¸æ“šé›†
    try:
        choice = input(f"\nè«‹é¸æ“‡æ•¸æ“šé›† [1-{len(datasets)}]: ").strip()
        index = int(choice) - 1
        if 0 <= index < len(datasets):
            selected_dataset = datasets[index]
        else:
            selected_dataset = datasets[0]
    except (ValueError, KeyboardInterrupt):
        selected_dataset = datasets[0]
    
    dataset_id = selected_dataset['id']
    dataset_name = selected_dataset['name']
    
    print(f"ğŸ“– é¸æ“‡æ•¸æ“šé›†: {dataset_name}")
    
    # è¨­ç½®èŠå¤©æ©Ÿå™¨äºº
    if not evaluator.setup_chatbot(dataset_id, dataset_name):
        print("âŒ èŠå¤©æ©Ÿå™¨äººè¨­ç½®å¤±æ•—")
        return False
    
    # ç²å–å•é¡Œæ•¸é‡
    try:
        num_questions = int(input(f"è«‹è¼¸å…¥æ¸¬è©¦å•é¡Œæ•¸é‡ [é è¨­: {DeepEvalConfig.DEFAULT_QUESTION_COUNT}]: ").strip() or str(DeepEvalConfig.DEFAULT_QUESTION_COUNT))
        num_questions = min(num_questions, DeepEvalConfig.MAX_QUESTION_COUNT)
    except ValueError:
        num_questions = DeepEvalConfig.DEFAULT_QUESTION_COUNT
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    print(f"\nğŸ“ ç”Ÿæˆ {num_questions} å€‹æ¸¬è©¦å•é¡Œ...")
    test_data = evaluator.generate_test_data_from_documents(dataset_id, num_questions)
    
    if not test_data:
        print("âŒ æ¸¬è©¦æ•¸æ“šç”Ÿæˆå¤±æ•—")
        return False
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_data)} å€‹æ¸¬è©¦å•é¡Œ:")
    for i, data in enumerate(test_data, 1):
        print(f"  {i}. {data['question']}")
    
    # ç¢ºèªé–‹å§‹è©•ä¼°
    input("\næŒ‰ Enter é–‹å§‹è©•ä¼°...")
    
    # åŸ·è¡Œè©•ä¼°
    results = evaluator.evaluate_test_cases(test_data)
    
    if not results:
        print("âŒ è©•ä¼°å¤±æ•—")
        return False
    
    # è©³ç´°çµæœé¡¯ç¤º
    print("\nğŸ“‹ è©³ç´°è©•ä¼°çµæœ:")
    print("=" * 60)
    
    for i, result in enumerate(results, 1):
        status = "âœ… é€šé" if result.passed else "âŒ å¤±æ•—"
        print(f"\n{i}. {status} | ç¸½åˆ†: {result.overall_score:.3f}")
        print(f"å•é¡Œ: {result.question}")
        print(f"å›ç­”: {result.actual_output[:150]}...")
        
        if result.metrics_scores:
            print("æŒ‡æ¨™åˆ†æ•¸:")
            for metric, score in result.metrics_scores.items():
                print(f"  - {metric}: {score:.3f}")
        
        if result.retrieval_context:
            print(f"æª¢ç´¢ä¸Šä¸‹æ–‡: {len(result.retrieval_context)} å€‹ç‰‡æ®µ")
    
    # ç”Ÿæˆä¸¦é¡¯ç¤ºå ±å‘Š
    report = evaluator.generate_report(results)
    print(f"\n{report}")
    
    # ä¿å­˜çµæœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"detailed_evaluation_{dataset_name}_{timestamp}.json"
    evaluator.save_results(results, filename)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜åˆ° {filename}")
    print("ğŸ‰ å–®ä¸€æ•¸æ“šé›†è©•ä¼°å®Œæˆï¼")
    
    return True

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ DeepEval æ¸¬è©¦é¸å–®")
    print("=" * 30)
    print("1. å¿«é€Ÿè©•ä¼° (æ‰€æœ‰æ•¸æ“šé›†)")
    print("2. è©³ç´°è©•ä¼° (å–®ä¸€æ•¸æ“šé›†)")
    print("3. é…ç½®æª¢æŸ¥")
    print("4. é€€å‡º")
    
    while True:
        try:
            choice = input("\nè«‹é¸æ“‡æ“ä½œ [1-4]: ").strip()
            
            if choice == '1':
                run_comprehensive_evaluation()
                break
            elif choice == '2':
                run_single_dataset_evaluation()
                break
            elif choice == '3':
                DeepEvalConfig.print_config_status()
            elif choice == '4':
                print("ğŸ‘‹ å†è¦‹ï¼")
                break
            else:
                print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–·ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()