# DeepEval è©•ä¼°ç³»çµ±ä½¿ç”¨æŒ‡å—

## ğŸ¯ æ¦‚è¿°

DeepEval æ˜¯ä¸€å€‹å°ˆæ¥­çš„ LLM è©•ä¼°æ¡†æ¶ï¼Œæœ¬é …ç›®å°‡å…¶æ•´åˆåˆ° RAGFlow èŠå¤©æ©Ÿå™¨äººä¸­ï¼Œæä¾›å…¨é¢çš„ RAG ç³»çµ±æ€§èƒ½è©•ä¼°ã€‚

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´

```bash
pip install -r requirements.txt
```

### 2. ç’°å¢ƒè¨­ç½®

```bash
# å¯é¸ï¼šè¨­ç½® OpenAI API å¯†é‘°ä»¥ä½¿ç”¨é«˜ç´šè©•ä¼°åŠŸèƒ½
export OPENAI_API_KEY="your-openai-api-key"

# ç¢ºä¿ RAGFlow API é…ç½®æ­£ç¢º
# ç·¨è¼¯ config.py æ–‡ä»¶
```

### 3. å¿«é€Ÿæ¼”ç¤º

```bash
# é‹è¡Œå¿«é€Ÿæ¼”ç¤º
python3 test/deepeval_demo.py
```

### 4. å®Œæ•´è©•ä¼°

```bash
# é‹è¡Œå®Œæ•´è©•ä¼°ç³»çµ±
python3 test/run_deepeval_test.py
```

## ğŸ“Š è©•ä¼°æŒ‡æ¨™èªªæ˜

### æ ¸å¿ƒæŒ‡æ¨™

| æŒ‡æ¨™ | èªªæ˜ | é–¾å€¼ | è©•ä¼°å…§å®¹ |
|------|------|------|----------|
| **Answer Relevancy** | å›ç­”ç›¸é—œæ€§ | â‰¥0.7 | å›ç­”æ˜¯å¦ç›´æ¥å›æ‡‰å•é¡Œ |
| **Faithfulness** | å›ç­”å¿ å¯¦åº¦ | â‰¥0.7 | å›ç­”æ˜¯å¦åŸºæ–¼æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ |
| **Contextual Precision** | ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦ | â‰¥0.7 | æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸é—œ |
| **Contextual Recall** | ä¸Šä¸‹æ–‡å¬å›ç‡ | â‰¥0.7 | æ˜¯å¦æª¢ç´¢åˆ°æ‰€æœ‰ç›¸é—œä¸Šä¸‹æ–‡ |
| **Hallucination** | å¹»è¦ºæª¢æ¸¬ | â‰¤0.3 | å›ç­”æ˜¯å¦åŒ…å«è™›å‡ä¿¡æ¯ |
| **Bias** | åè¦‹æª¢æ¸¬ | â‰¤0.5 | å›ç­”æ˜¯å¦å­˜åœ¨åè¦‹ |

### æ³•å¾‹å°ˆæ¥­æŒ‡æ¨™

- **æ³•æ¢ç²¾ç¢ºåº¦**: æ³•æ¢å¼•ç”¨çš„æº–ç¢ºæ€§
- **åˆ¤ä¾‹ç›¸é—œæ€§**: ç›¸é—œåˆ¤ä¾‹çš„åŒ¹é…ç¨‹åº¦
- **å°ˆæ¥­è¡“èªä¸€è‡´æ€§**: æ³•å¾‹è¡“èªä½¿ç”¨çš„æ­£ç¢ºæ€§
- **è€ƒè©¦å°å‘æ€§**: å›ç­”å°è€ƒè©¦çš„å¯¦ç”¨æ€§

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬è©•ä¼°æµç¨‹

```python
from deepeval_integration import RAGFlowEvaluator

# 1. å‰µå»ºè©•ä¼°å™¨
evaluator = RAGFlowEvaluator()

# 2. è¨­ç½®æ•¸æ“šé›†
dataset_id = "your-dataset-id"
evaluator.setup_chatbot(dataset_id, "æ•¸æ“šé›†åç¨±")

# 3. ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
test_data = evaluator.generate_test_data_from_documents(dataset_id, 10)

# 4. åŸ·è¡Œè©•ä¼°
results = evaluator.evaluate_test_cases(test_data)

# 5. ç”Ÿæˆå ±å‘Š
report = evaluator.generate_report(results)
print(report)

# 6. ä¿å­˜çµæœ
evaluator.save_results(results, "evaluation_results.json")
```

### è‡ªå®šç¾©æ¸¬è©¦æ•¸æ“š

```python
# æ‰‹å‹•å‰µå»ºæ¸¬è©¦æ•¸æ“š
custom_test_data = [
    {
        'id': 'custom_1',
        'question': 'ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ',
        'expected_answer': 'æ†²æ³•çš„åŸºæœ¬åŸå‰‡åŒ…æ‹¬äººæ°‘ä¸»æ¬Šã€æ¬ŠåŠ›åˆ†ç«‹ã€åŸºæœ¬äººæ¬Šä¿éšœç­‰ã€‚',
        'context': 'æ†²æ³•ç›¸é—œå…§å®¹',
        'source': 'manual'
    }
]

# ä½¿ç”¨è‡ªå®šç¾©æ•¸æ“šè©•ä¼°
results = evaluator.evaluate_test_cases(custom_test_data)
```

### é…ç½®è‡ªå®šç¾©æŒ‡æ¨™

```python
from deepeval.metrics import AnswerRelevancyMetric

# å‰µå»ºè‡ªå®šç¾©æŒ‡æ¨™
custom_metric = AnswerRelevancyMetric(
    threshold=0.8,  # æé«˜é–¾å€¼
    model="gpt-4",  # ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹
    include_reason=True  # åŒ…å«è©•ä¼°ç†ç”±
)

# æ·»åŠ åˆ°è©•ä¼°å™¨
evaluator.metrics['custom_relevancy'] = custom_metric
```

## ğŸ“ å•ç­”æ•¸æ“šç”Ÿæˆ

### è‡ªå‹•ç”Ÿæˆç­–ç•¥

1. **åŸºæ–¼æ–‡æª”å…§å®¹**: å¾æ•¸æ“šé›†æ–‡æª”ä¸­æå–é—œéµä¿¡æ¯ç”Ÿæˆå•é¡Œ
2. **é ˜åŸŸç‰¹åŒ–**: æ ¹æ“šæ•¸æ“šé›†é¡å‹ï¼ˆæ³•å¾‹ã€æŠ€è¡“ç­‰ï¼‰ç”Ÿæˆå°ˆæ¥­å•é¡Œ
3. **é›£åº¦åˆ†ç´š**: ç”Ÿæˆä¸åŒé›£åº¦ç´šåˆ¥çš„å•é¡Œ
4. **å¤šæ¨£åŒ–**: ç¢ºä¿å•é¡Œé¡å‹çš„å¤šæ¨£æ€§

### æ³•å¾‹é ˜åŸŸå•é¡Œç”Ÿæˆ

```python
# æ³•å¾‹å°ˆæ¥­å•é¡Œæ¨¡æ¿
legal_templates = [
    "ä»€éº¼æ˜¯{concept}çš„åŸºæœ¬åŸå‰‡ï¼Ÿ",
    "{law_area}ä¸­çš„{principle}å¦‚ä½•ç†è§£ï¼Ÿ",
    "è«‹è§£é‡‹{legal_term}çš„å®šç¾©å’Œé©ç”¨ç¯„åœ",
    "{case_type}æ¡ˆä»¶çš„è™•ç†ç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ"
]

# è‡ªå‹•å¡«å……æ¨¡æ¿
concepts = ["æ†²æ³•", "æ°‘æ³•", "åˆ‘æ³•", "è¡Œæ”¿æ³•"]
questions = generate_from_templates(legal_templates, concepts)
```

### æŠ€è¡“é ˜åŸŸå•é¡Œç”Ÿæˆ

```python
# æŠ€è¡“å°ˆæ¥­å•é¡Œæ¨¡æ¿
tech_templates = [
    "ä»€éº¼æ˜¯{technology}ï¼Ÿ",
    "å¦‚ä½•å¯¦ç¾{feature}åŠŸèƒ½ï¼Ÿ",
    "{framework}çš„ä¸»è¦ç‰¹é»æ˜¯ä»€éº¼ï¼Ÿ",
    "{concept}çš„å·¥ä½œåŸç†æ˜¯ä»€éº¼ï¼Ÿ"
]

# æŠ€è¡“æ¦‚å¿µ
technologies = ["API", "REST", "GraphQL", "å¾®æœå‹™"]
questions = generate_from_templates(tech_templates, technologies)
```

## ğŸ“Š è©•ä¼°å ±å‘Šè§£è®€

### å ±å‘Šçµæ§‹

```
ğŸ“Š RAGFlow ç³»çµ±è©•ä¼°å ±å‘Š
==================================================

ğŸ“ˆ æ•´é«”çµ±è¨ˆ:
- ç¸½æ¸¬è©¦æ¡ˆä¾‹: 10
- é€šéæ¡ˆä¾‹: 8
- é€šéç‡: 80.0%
- å¹³å‡åˆ†æ•¸: 0.756

ğŸ“‹ æŒ‡æ¨™è©³æƒ…:
- answer_relevancy: å¹³å‡ 0.823 (ç¯„åœ: 0.654 - 0.945)
- faithfulness: å¹³å‡ 0.789 (ç¯„åœ: 0.612 - 0.891)

ğŸ” è©³ç´°çµæœ:
1. âœ… é€šé | åˆ†æ•¸: 0.856 | ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ...
2. âŒ å¤±æ•— | åˆ†æ•¸: 0.634 | æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ...
```

### åˆ†æ•¸è§£é‡‹

- **0.8-1.0**: å„ªç§€ - ç³»çµ±è¡¨ç¾éå¸¸å¥½
- **0.6-0.8**: è‰¯å¥½ - ç³»çµ±è¡¨ç¾å¯æ¥å—ï¼Œæœ‰æ”¹é€²ç©ºé–“
- **0.4-0.6**: ä¸€èˆ¬ - ç³»çµ±éœ€è¦å„ªåŒ–
- **0.0-0.4**: å·® - ç³»çµ±å­˜åœ¨åš´é‡å•é¡Œ

### æ”¹é€²å»ºè­°

æ ¹æ“šè©•ä¼°çµæœï¼Œå¯ä»¥è€ƒæ…®ä»¥ä¸‹æ”¹é€²æ–¹å‘ï¼š

1. **ä½ç›¸é—œæ€§åˆ†æ•¸**: æ”¹é€²æª¢ç´¢ç®—æ³•æˆ–æ“´å……çŸ¥è­˜åº«
2. **ä½å¿ å¯¦åº¦åˆ†æ•¸**: å„ªåŒ–å›ç­”ç”Ÿæˆé‚è¼¯ï¼Œç¢ºä¿åŸºæ–¼æª¢ç´¢å…§å®¹
3. **é«˜å¹»è¦ºåˆ†æ•¸**: åŠ å¼·äº‹å¯¦æª¢æŸ¥æ©Ÿåˆ¶
4. **é«˜åè¦‹åˆ†æ•¸**: å¯©æŸ¥è¨“ç·´æ•¸æ“šå’Œå›ç­”æ¨¡æ¿

## ğŸ› ï¸ é€²éšåŠŸèƒ½

### æ‰¹é‡è©•ä¼°

```python
# è©•ä¼°å¤šå€‹æ•¸æ“šé›†
datasets = evaluator.client.list_datasets()['data']

for dataset in datasets:
    evaluator.setup_chatbot(dataset['id'], dataset['name'])
    test_data = evaluator.generate_test_data_from_documents(dataset['id'], 5)
    results = evaluator.evaluate_test_cases(test_data)
    
    # ä¿å­˜æ¯å€‹æ•¸æ“šé›†çš„çµæœ
    filename = f"eval_{dataset['name']}.json"
    evaluator.save_results(results, filename)
```

### è‡ªå®šç¾©è©•ä¼°æµç¨‹

```python
class CustomEvaluator(RAGFlowEvaluator):
    def custom_evaluation_step(self, test_case, result):
        """è‡ªå®šç¾©è©•ä¼°æ­¥é©Ÿ"""
        # æ·»åŠ ç‰¹å®šé ˜åŸŸçš„è©•ä¼°é‚è¼¯
        pass
    
    def generate_custom_report(self, results):
        """ç”Ÿæˆè‡ªå®šç¾©å ±å‘Š"""
        # å¯¦ç¾ç‰¹å®šçš„å ±å‘Šæ ¼å¼
        pass
```

### æŒçºŒè©•ä¼°

```python
import schedule
import time

def scheduled_evaluation():
    """å®šæœŸè©•ä¼°ä»»å‹™"""
    evaluator = RAGFlowEvaluator()
    # åŸ·è¡Œè©•ä¼°é‚è¼¯
    pass

# æ¯å¤©åŸ·è¡Œä¸€æ¬¡è©•ä¼°
schedule.every().day.at("02:00").do(scheduled_evaluation)

while True:
    schedule.run_pending()
    time.sleep(1)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **OpenAI API éŒ¯èª¤**
   ```
   è§£æ±ºæ–¹æ¡ˆ: æª¢æŸ¥ API å¯†é‘°è¨­ç½®å’Œç¶²çµ¡é€£æ¥
   export OPENAI_API_KEY="your-key"
   ```

2. **RAGFlow é€£æ¥å¤±æ•—**
   ```
   è§£æ±ºæ–¹æ¡ˆ: æª¢æŸ¥ config.py ä¸­çš„ API é…ç½®
   ```

3. **è©•ä¼°æŒ‡æ¨™è¨ˆç®—å¤±æ•—**
   ```
   è§£æ±ºæ–¹æ¡ˆ: ç¢ºä¿æ¸¬è©¦æ•¸æ“šæ ¼å¼æ­£ç¢ºï¼Œæª¢æŸ¥ä¾è³´ç‰ˆæœ¬
   ```

4. **å…§å­˜ä¸è¶³**
   ```
   è§£æ±ºæ–¹æ¡ˆ: æ¸›å°‘æ‰¹é‡å¤§å°æˆ–ä½¿ç”¨æ›´è¼•é‡çš„æ¨¡å‹
   ```

### èª¿è©¦æ¨¡å¼

```python
import logging

# å•Ÿç”¨è©³ç´°æ—¥èªŒ
logging.basicConfig(level=logging.DEBUG)

# ä½¿ç”¨èª¿è©¦æ¨¡å¼
evaluator = RAGFlowEvaluator()
evaluator.debug_mode = True
```

## ğŸ“š åƒè€ƒè³‡æº

- [DeepEval å®˜æ–¹æ–‡æª”](https://docs.confident-ai.com/)
- [RAGFlow API æ–‡æª”](https://ragflow.io/docs/dev/python_api_reference)
- [OpenAI API æ–‡æª”](https://platform.openai.com/docs)

## ğŸ¤ è²¢ç»æŒ‡å—

æ­¡è¿æäº¤å•é¡Œå ±å‘Šå’ŒåŠŸèƒ½è«‹æ±‚ï¼š

1. Fork é …ç›®
2. å‰µå»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. ç™¼èµ· Pull Request

## ğŸ“„ è¨±å¯è­‰

MIT License - è©³è¦‹ LICENSE æ–‡ä»¶

---

**æœ€å¾Œæ›´æ–°**: 2025å¹´8æœˆ3æ—¥  
**ç‰ˆæœ¬**: v1.0.0  
**ç¶­è­·è€…**: Kiro AI Assistant