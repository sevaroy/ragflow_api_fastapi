"""
è©•ä¼°é é¢çµ„ä»¶
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime
from evaluators.ragas import RAGASEvaluator


def render_evaluation_steps(current_step):
    """æ¸²æŸ“è©•ä¼°æ­¥é©ŸæŒ‡ç¤ºå™¨"""
    step_cols = st.columns(4)
    steps = [
        {"num": 1, "name": "âš™ï¸ é…ç½®è¨­å®š", "desc": "é¸æ“‡æ•¸æ“šé›†å’ŒæŒ‡æ¨™"},
        {"num": 2, "name": "ğŸ“ æ•¸æ“šæº–å‚™", "desc": "ç”Ÿæˆæ¸¬è©¦å•é¡Œ"},
        {"num": 3, "name": "ğŸ”„ åŸ·è¡Œè©•ä¼°", "desc": "é‹è¡Œ RAGAS è©•ä¼°"},
        {"num": 4, "name": "ğŸ“ˆ çµæœåˆ†æ", "desc": "æŸ¥çœ‹è©•ä¼°çµæœ"}
    ]
    
    for i, (col, step) in enumerate(zip(step_cols, steps)):
        with col:
            if step["num"] == current_step:
                st.success(f"**{step['name']}**\n\n{step['desc']}")
            elif step["num"] < current_step:
                st.info(f"âœ… **{step['name']}**")
            else:
                st.write(f"â³ **{step['name']}**")


def render_evaluation_config():
    """æ¸²æŸ“è©•ä¼°é…ç½®"""
    st.markdown("### âš™ï¸ è©•ä¼°é…ç½®")
    
    # æª¢æŸ¥ API é€£æ¥
    if not st.session_state.api_connected:
        st.warning("âš ï¸ è«‹å…ˆåœ¨å´é‚Šæ¬„æª¢æŸ¥ API é€£æ¥")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“š æ•¸æ“šé›†é¸æ“‡**")
        
        # ç²å–å¯ç”¨æ•¸æ“šé›†
        datasets_result = st.session_state.client.get_datasets()
        if datasets_result['success']:
            datasets = datasets_result['data']
            if datasets:
                selected_dataset = st.selectbox(
                    "é¸æ“‡çŸ¥è­˜åº«",
                    options=datasets,
                    format_func=lambda x: f"{x.get('name', 'Unknown')} ({x.get('document_count', 0)} æ–‡æª”)",
                    key="eval_dataset"
                )
                # selected_dataset æœƒè‡ªå‹•å­˜å„²åœ¨ st.session_state.eval_dataset ä¸­
            else:
                st.warning("æ²’æœ‰å¯ç”¨çš„æ•¸æ“šé›†")
                return
        else:
            st.error(f"è¼‰å…¥æ•¸æ“šé›†å¤±æ•—: {datasets_result['error']}")
            return
        
        st.markdown("**ğŸ“Š æ¸¬è©¦åƒæ•¸**")
        num_questions = st.slider("æ¸¬è©¦å•é¡Œæ•¸é‡", 5, 50, 20, key="eval_num_questions")
        threshold = st.slider("é€šéé–¾å€¼", 0.5, 0.9, 0.7, step=0.05, key="eval_threshold")
    
    with col2:
        st.markdown("**ğŸ¯ è©•ä¼°æŒ‡æ¨™é¸æ“‡**")
        
        try:
            from ragas import EvaluationDataset
            RAGAS_AVAILABLE = True
        except ImportError:
            RAGAS_AVAILABLE = False
        
        if RAGAS_AVAILABLE:
            available_metrics = [
                'faithfulness',
                'answer_relevancy', 
                'context_precision',
                'context_recall',
                'answer_similarity',
                'answer_correctness'
            ]
        else:
            st.error("âŒ RAGAS æœªå®‰è£ï¼Œç„¡æ³•é€²è¡Œè©•ä¼°")
            st.info("ğŸ’¡ è«‹å®‰è£ RAGAS: pip install ragas")
            return
        
        metric_labels = {
            'faithfulness': 'ğŸ” å¿ å¯¦åº¦',
            'answer_relevancy': 'ğŸ¯ ç­”æ¡ˆç›¸é—œæ€§',
            'context_precision': 'ğŸ“ ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦',
            'context_recall': 'ğŸ“‹ ä¸Šä¸‹æ–‡å¬å›ç‡',
            'answer_similarity': 'ğŸ”„ ç­”æ¡ˆç›¸ä¼¼åº¦',
            'answer_correctness': 'âœ… ç­”æ¡ˆæ­£ç¢ºæ€§'
        }
        
        selected_metrics = []
        for metric in available_metrics:
            if st.checkbox(
                metric_labels.get(metric, metric),
                value=metric in ['faithfulness', 'answer_relevancy', 'context_precision'],
                key=f"metric_{metric}"
            ):
                selected_metrics.append(metric)
        
        st.session_state.eval_metrics = selected_metrics
        
        if not selected_metrics:
            st.warning("âš ï¸ è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è©•ä¼°æŒ‡æ¨™")
            return
        
        st.markdown("**ğŸ“‹ å•é¡Œé¡å‹**")
        question_types = st.multiselect(
            "é¸æ“‡å•é¡Œé¡å‹",
            ["äº‹å¯¦æŸ¥è©¢", "æ¦‚å¿µè§£é‡‹", "æ¡ˆä¾‹åˆ†æ"],
            default=["äº‹å¯¦æŸ¥è©¢", "æ¦‚å¿µè§£é‡‹"],
            key="eval_question_types"
        )
        # question_types æœƒè‡ªå‹•å­˜å„²åœ¨ st.session_state.eval_question_types ä¸­
    
    st.divider()
    
    # é…ç½®æ‘˜è¦
    if hasattr(st.session_state, 'eval_dataset') and selected_metrics:
        st.markdown("**ğŸ“‹ é…ç½®æ‘˜è¦**")
        st.info(f"""
        - **æ•¸æ“šé›†**: {st.session_state.eval_dataset.get('name', 'Unknown')}
        - **æ¸¬è©¦å•é¡Œ**: {num_questions} å€‹
        - **è©•ä¼°æŒ‡æ¨™**: {len(selected_metrics)} å€‹ ({', '.join(selected_metrics)})
        - **å•é¡Œé¡å‹**: {', '.join(question_types)}
        - **é€šéé–¾å€¼**: {threshold}
        """)
        
        if st.button("â¡ï¸ ä¸‹ä¸€æ­¥ï¼šæº–å‚™æ•¸æ“š", type="primary", use_container_width=True):
            st.session_state.evaluation_step = 2
            st.rerun()


def render_data_preparation():
    """æ¸²æŸ“æ•¸æ“šæº–å‚™éšæ®µ"""
    st.markdown("### ğŸ“ æ•¸æ“šæº–å‚™")
    
    # æª¢æŸ¥é…ç½®
    if not hasattr(st.session_state, 'eval_dataset'):
        st.error("âŒ è«‹å…ˆå®Œæˆè©•ä¼°é…ç½®")
        return
    
    st.info("ğŸ”„ æ­£åœ¨æº–å‚™æ¸¬è©¦æ•¸æ“š...")
    
    # é¡¯ç¤ºé…ç½®ä¿¡æ¯
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“Š è©•ä¼°é…ç½®**")
        st.write(f"æ•¸æ“šé›†: {st.session_state.eval_dataset.get('name', 'Unknown')}")
        st.write(f"å•é¡Œæ•¸é‡: {st.session_state.get('eval_num_questions', 20)}")
        st.write(f"è©•ä¼°æŒ‡æ¨™: {len(st.session_state.get('eval_metrics', []))} å€‹")
    
    with col2:
        st.markdown("**ğŸ¯ ç”Ÿæˆç­–ç•¥**")
        st.write(f"å•é¡Œé¡å‹: {', '.join(st.session_state.get('eval_question_types', []))}")
        st.write(f"é€šéé–¾å€¼: {st.session_state.get('eval_threshold', 0.7)}")
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“šé è¦½
    if st.button("ğŸ” é è¦½æ¸¬è©¦å•é¡Œ", use_container_width=True):
        with st.spinner("ç”Ÿæˆå•é¡Œé è¦½..."):
            evaluator = RAGASEvaluator(st.session_state.client)
            sample_questions = evaluator.generate_test_questions(
                dataset_id=st.session_state.eval_dataset['id'],
                num_questions=5,
                question_types=st.session_state.get('eval_question_types', [])
            )
            
            st.markdown("**ğŸ“‹ å•é¡Œé è¦½**")
            for i, q in enumerate(sample_questions, 1):
                st.write(f"{i}. {q['question']} *({q['question_type']})*")
    
    st.divider()
    
    # å°èˆªæŒ‰éˆ•
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("â¬…ï¸ è¿”å›é…ç½®", use_container_width=True):
            st.session_state.evaluation_step = 1
            st.rerun()
    
    with col2:
        if st.button("â¡ï¸ é–‹å§‹è©•ä¼°", type="primary", use_container_width=True):
            st.session_state.evaluation_step = 3
            st.rerun()


def render_evaluation_execution():
    """æ¸²æŸ“è©•ä¼°åŸ·è¡Œéšæ®µ"""
    st.markdown("### ğŸ”„ åŸ·è¡Œè©•ä¼°")
    
    # åˆå§‹åŒ–è©•ä¼°ç‹€æ…‹
    if 'evaluation_progress' not in st.session_state:
        st.session_state.evaluation_progress = {
            'status': 'starting',
            'current_step': 0,
            'total_steps': 4,
            'message': 'æº–å‚™é–‹å§‹è©•ä¼°...'
        }
    
    progress = st.session_state.evaluation_progress
    
    # é€²åº¦é¡¯ç¤º
    progress_bar = st.progress(progress['current_step'] / progress['total_steps'])
    status_text = st.empty()
    status_text.info(f"ğŸ“Š {progress['message']}")
    
    # åŸ·è¡Œè©•ä¼°
    if progress['status'] == 'starting':
        st.session_state.evaluation_progress.update({
            'status': 'generating_questions',
            'current_step': 1,
            'message': 'ç”Ÿæˆæ¸¬è©¦å•é¡Œ...'
        })
        st.rerun()
    
    elif progress['status'] == 'generating_questions':
        with st.spinner("ç”Ÿæˆæ¸¬è©¦å•é¡Œ..."):
            evaluator = RAGASEvaluator(st.session_state.client)
            test_cases = evaluator.generate_test_questions(
                dataset_id=st.session_state.eval_dataset['id'],
                num_questions=st.session_state.get('eval_num_questions', 20),
                question_types=st.session_state.get('eval_question_types', [])
            )
            st.session_state.test_cases = test_cases
        
        st.session_state.evaluation_progress.update({
            'status': 'getting_responses',
            'current_step': 2,
            'message': 'ç²å– RAG ç³»çµ±å›ç­”...'
        })
        st.rerun()
    
    elif progress['status'] == 'getting_responses':
        with st.spinner("ç²å– RAG ç³»çµ±å›ç­”..."):
            evaluator = RAGASEvaluator(st.session_state.client)
            enriched_cases = evaluator.get_rag_responses(st.session_state.test_cases)
            st.session_state.enriched_cases = enriched_cases
        
        st.session_state.evaluation_progress.update({
            'status': 'evaluating',
            'current_step': 3,
            'message': 'åŸ·è¡Œ RAGAS è©•ä¼°...'
        })
        st.rerun()
    
    elif progress['status'] == 'evaluating':
        with st.spinner("åŸ·è¡Œ RAGAS è©•ä¼°..."):
            evaluator = RAGASEvaluator(st.session_state.client)
            results = evaluator.evaluate_with_ragas(
                st.session_state.enriched_cases,
                st.session_state.get('eval_metrics', [])
            )
            st.session_state.evaluation_results = results
        
        st.session_state.evaluation_progress.update({
            'status': 'completed',
            'current_step': 4,
            'message': 'è©•ä¼°å®Œæˆï¼'
        })
        st.success("âœ… è©•ä¼°å®Œæˆï¼")
        
        if st.button("â¡ï¸ æŸ¥çœ‹çµæœ", type="primary", use_container_width=True):
            st.session_state.evaluation_step = 4
            st.rerun()


def render_evaluation_results():
    """æ¸²æŸ“è©•ä¼°çµæœ"""
    st.markdown("### ğŸ“ˆ è©•ä¼°çµæœ")
    
    if not hasattr(st.session_state, 'evaluation_results'):
        st.error("âŒ æ²’æœ‰è©•ä¼°çµæœ")
        return
    
    results = st.session_state.evaluation_results
    
    if not results.get('success'):
        st.error(f"âŒ è©•ä¼°å¤±æ•—: {results.get('error', 'Unknown error')}")
        return
    
    # æ‘˜è¦çµ±è¨ˆ
    summary = results.get('summary', {})
    
    st.markdown("#### ğŸ“Š è©•ä¼°æ‘˜è¦")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ç¸½æ¸¬è©¦æ¡ˆä¾‹",
            summary.get('total_cases', 0)
        )
    
    with col2:
        st.metric(
            "é€šéæ¡ˆä¾‹",
            summary.get('passed_cases', 0),
            f"{summary.get('pass_rate', 0):.1%}"
        )
    
    with col3:
        st.metric(
            "å¹³å‡åˆ†æ•¸",
            f"{summary.get('avg_score', 0):.3f}"
        )
    
    with col4:
        st.metric(
            "åˆ†æ•¸ç¯„åœ",
            f"{summary.get('min_score', 0):.2f} - {summary.get('max_score', 0):.2f}"
        )
    
    # æŒ‡æ¨™è©³æƒ…
    if 'metrics_stats' in summary:
        st.markdown("#### ğŸ“ˆ å„æŒ‡æ¨™çµ±è¨ˆ")
        
        metrics_data = []
        for metric, stats in summary['metrics_stats'].items():
            metrics_data.append({
                'metric': metric,
                'mean': stats['mean'],
                'min': stats['min'],
                'max': stats['max'],
                'std': stats['std']
            })
        
        if metrics_data:
            df = pd.DataFrame(metrics_data)
            st.dataframe(df, use_container_width=True)
    
    # è©³ç´°çµæœ
    st.markdown("#### ğŸ“‹ è©³ç´°çµæœ")
    
    detailed_results = results.get('results', [])
    
    if detailed_results:
        # ç¯©é¸é¸é …
        col1, col2 = st.columns(2)
        
        with col1:
            show_only_failed = st.checkbox("åªé¡¯ç¤ºå¤±æ•—æ¡ˆä¾‹")
        
        with col2:
            min_score_filter = st.slider("æœ€ä½åˆ†æ•¸ç¯©é¸", 0.0, 1.0, 0.0, 0.1)
        
        # æ‡‰ç”¨ç¯©é¸
        filtered_results = detailed_results
        if show_only_failed:
            filtered_results = [r for r in filtered_results if not r.get('passed', True)]
        
        filtered_results = [r for r in filtered_results if r.get('overall_score', 0) >= min_score_filter]
        
        st.write(f"é¡¯ç¤º {len(filtered_results)} / {len(detailed_results)} å€‹çµæœ")
        
        # é¡¯ç¤ºçµæœ
        for result in filtered_results[:10]:  # åªé¡¯ç¤ºå‰10å€‹
            status = "âœ… é€šé" if result.get('passed', False) else "âŒ å¤±æ•—"
            
            with st.expander(f"{status} - {result.get('test_id', 'Unknown')} (åˆ†æ•¸: {result.get('overall_score', 0):.3f})"):
                st.write(f"**å•é¡Œ**: {result.get('question', 'N/A')}")
                st.write(f"**å›ç­”**: {result.get('actual_answer', 'N/A')[:200]}...")
                
                # é¡¯ç¤ºå„é …æŒ‡æ¨™åˆ†æ•¸
                metrics_cols = st.columns(3)
                for i, (metric, score) in enumerate([(k, v) for k, v in result.items() if k in st.session_state.get('eval_metrics', [])]):
                    with metrics_cols[i % 3]:
                        st.metric(metric, f"{score:.3f}")
    
    # æ“ä½œæŒ‰éˆ•
    st.divider()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ”„ é‡æ–°è©•ä¼°", use_container_width=True):
            # é‡ç½®è©•ä¼°ç‹€æ…‹
            if 'evaluation_progress' in st.session_state:
                del st.session_state.evaluation_progress
            st.session_state.evaluation_step = 1
            st.rerun()
    
    with col2:
        if st.button("ğŸ“Š æŸ¥çœ‹è©³ç´°åˆ†æ", use_container_width=True):
            st.session_state.current_page = 'results'
            st.rerun()
    
    with col3:
        # ä¿å­˜çµæœ
        if st.button("ğŸ’¾ ä¿å­˜çµæœ", use_container_width=True):
            evaluator = RAGASEvaluator(st.session_state.client)
            filename = evaluator.save_results(
                results, 
                st.session_state.eval_dataset.get('name', 'unknown')
            )
            if filename:
                st.success(f"âœ… çµæœå·²ä¿å­˜è‡³: {filename}")
            else:
                st.error("âŒ ä¿å­˜å¤±æ•—")
