#!/usr/bin/env python3
"""
DeepEval é…ç½®æ–‡ä»¶
ç®¡ç†è©•ä¼°ç³»çµ±çš„å„ç¨®è¨­ç½®å’Œåƒæ•¸
"""

import os
from typing import Dict, List, Any

class DeepEvalConfig:
    """DeepEval é…ç½®é¡"""
    
    # API è¨­ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
    
    # è©•ä¼°æŒ‡æ¨™é–¾å€¼
    METRIC_THRESHOLDS = {
        'answer_relevancy': 0.7,
        'faithfulness': 0.7,
        'contextual_precision': 0.7,
        'contextual_recall': 0.7,
        'hallucination': 0.3,  # è¶Šä½è¶Šå¥½
        'bias': 0.5
    }
    
    # åˆ†éšæ®µè©•ä¼°ç­–ç•¥
    EVALUATION_STAGES = {
        'quick': ['answer_relevancy'],  # å¿«é€Ÿè©•ä¼°ï¼š1å€‹æŒ‡æ¨™
        'standard': ['answer_relevancy', 'faithfulness'],  # æ¨™æº–è©•ä¼°ï¼š2å€‹æŒ‡æ¨™
        'comprehensive': ['answer_relevancy', 'faithfulness', 'hallucination'],  # å…¨é¢è©•ä¼°ï¼š3å€‹æŒ‡æ¨™
        'full': ['answer_relevancy', 'faithfulness', 'contextual_precision', 'contextual_recall', 'hallucination', 'bias']  # å®Œæ•´è©•ä¼°ï¼šæ‰€æœ‰æŒ‡æ¨™
    }
    
    # æ¸¬è©¦æ•¸æ“šç”Ÿæˆè¨­ç½®
    DEFAULT_QUESTION_COUNT = 10
    MAX_QUESTION_COUNT = 50
    
    # æ³•å¾‹é ˜åŸŸæ¸¬è©¦å•é¡Œæ¨¡æ¿
    LEGAL_QUESTION_TEMPLATES = [
        "ä»€éº¼æ˜¯{concept}çš„åŸºæœ¬åŸå‰‡ï¼Ÿ",
        "{law_area}ä¸­çš„{principle}å¦‚ä½•ç†è§£ï¼Ÿ",
        "è«‹è§£é‡‹{legal_term}çš„å®šç¾©å’Œé©ç”¨ç¯„åœ",
        "{case_type}æ¡ˆä»¶çš„è™•ç†ç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ",
        "{legal_document}çš„ä¸»è¦å…§å®¹åŒ…æ‹¬å“ªäº›ï¼Ÿ"
    ]
    
    # æŠ€è¡“é ˜åŸŸæ¸¬è©¦å•é¡Œæ¨¡æ¿
    TECH_QUESTION_TEMPLATES = [
        "ä»€éº¼æ˜¯{technology}ï¼Ÿ",
        "å¦‚ä½•å¯¦ç¾{feature}åŠŸèƒ½ï¼Ÿ",
        "{framework}çš„ä¸»è¦ç‰¹é»æ˜¯ä»€éº¼ï¼Ÿ",
        "{concept}çš„å·¥ä½œåŸç†æ˜¯ä»€éº¼ï¼Ÿ",
        "ä½¿ç”¨{tool}æ™‚éœ€è¦æ³¨æ„ä»€éº¼ï¼Ÿ"
    ]
    
    # é€šç”¨æ¸¬è©¦å•é¡Œæ¨¡æ¿
    GENERAL_QUESTION_TEMPLATES = [
        "é€™å€‹ä¸»é¡Œçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€éº¼ï¼Ÿ",
        "æœ‰å“ªäº›é‡è¦çš„æ³¨æ„äº‹é …ï¼Ÿ",
        "æœ€ä½³å¯¦è¸æ˜¯ä»€éº¼ï¼Ÿ",
        "å¸¸è¦‹å•é¡Œæœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•é–‹å§‹å­¸ç¿’é€™å€‹é ˜åŸŸï¼Ÿ"
    ]
    
    # è©•ä¼°å ±å‘Šè¨­ç½®
    REPORT_CONFIG = {
        'include_detailed_scores': True,
        'include_failed_cases': True,
        'max_output_length': 200,
        'export_formats': ['json', 'csv', 'html']
    }
    
    # æ³•å¾‹å°ˆæ¥­è©å½™
    LEGAL_CONCEPTS = [
        "æ†²æ³•", "è¡Œæ”¿æ³•", "æ°‘æ³•", "åˆ‘æ³•", "å•†æ³•",
        "å¥‘ç´„è‡ªç”±", "ç½ªåˆ‘æ³•å®š", "æ¯”ä¾‹åŸå‰‡", "ä¿¡è³´ä¿è­·",
        "åŸºæœ¬äººæ¬Š", "æ¬ŠåŠ›åˆ†ç«‹", "æ³•æ²»åœ‹å®¶"
    ]
    
    LEGAL_TERMS = [
        "æ³•å¾‹è¡Œç‚º", "ç‰©æ¬Š", "å‚µæ¬Š", "è¦ªå±¬é—œä¿‚", "ç¹¼æ‰¿",
        "çŠ¯ç½ªæ§‹æˆè¦ä»¶", "åˆ‘ç½°", "ç·©åˆ‘", "å‡é‡‹",
        "è¡Œæ”¿è™•åˆ†", "è¡Œæ”¿æ•‘æ¿Ÿ", "åœ‹å®¶è³ å„Ÿ"
    ]
    
    # æŠ€è¡“å°ˆæ¥­è©å½™
    TECH_CONCEPTS = [
        "API", "REST", "GraphQL", "å¾®æœå‹™", "å®¹å™¨åŒ–",
        "æ©Ÿå™¨å­¸ç¿’", "æ·±åº¦å­¸ç¿’", "è‡ªç„¶èªè¨€è™•ç†",
        "å€å¡Šéˆ", "é›²ç«¯é‹ç®—", "DevOps"
    ]
    
    @classmethod
    def get_question_templates(cls, domain: str) -> List[str]:
        """æ ¹æ“šé ˜åŸŸç²å–å•é¡Œæ¨¡æ¿"""
        if "æ³•å¾‹" in domain or "legal" in domain.lower():
            return cls.LEGAL_QUESTION_TEMPLATES
        elif "æŠ€è¡“" in domain or "tech" in domain.lower():
            return cls.TECH_QUESTION_TEMPLATES
        else:
            return cls.GENERAL_QUESTION_TEMPLATES
    
    @classmethod
    def get_domain_concepts(cls, domain: str) -> List[str]:
        """æ ¹æ“šé ˜åŸŸç²å–å°ˆæ¥­æ¦‚å¿µ"""
        if "æ³•å¾‹" in domain or "legal" in domain.lower():
            return cls.LEGAL_CONCEPTS + cls.LEGAL_TERMS
        elif "æŠ€è¡“" in domain or "tech" in domain.lower():
            return cls.TECH_CONCEPTS
        else:
            return ["æ ¸å¿ƒæ¦‚å¿µ", "åŸºæœ¬åŸç†", "å¯¦å‹™æ‡‰ç”¨", "æœ€ä½³å¯¦è¸"]
    
    @classmethod
    def validate_config(cls) -> Dict[str, Any]:
        """é©—è­‰é…ç½®è¨­ç½®"""
        issues = []
        warnings = []
        
        # æª¢æŸ¥ OpenAI API å¯†é‘°
        if not cls.OPENAI_API_KEY:
            warnings.append("æœªè¨­ç½® OPENAI_API_KEYï¼ŒæŸäº›é«˜ç´šåŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        
        # æª¢æŸ¥é–¾å€¼è¨­ç½®
        for metric, threshold in cls.METRIC_THRESHOLDS.items():
            if not 0 <= threshold <= 1:
                issues.append(f"æŒ‡æ¨™ {metric} çš„é–¾å€¼ {threshold} ä¸åœ¨æœ‰æ•ˆç¯„åœ [0, 1] å…§")
        
        # æª¢æŸ¥å•é¡Œæ•¸é‡è¨­ç½®
        if cls.DEFAULT_QUESTION_COUNT > cls.MAX_QUESTION_COUNT:
            issues.append("é è¨­å•é¡Œæ•¸é‡ä¸èƒ½è¶…éæœ€å¤§å•é¡Œæ•¸é‡")
        
        return {
            'valid': len(issues) == 0,
            'issues': issues,
            'warnings': warnings
        }
    
    @classmethod
    def print_config_status(cls):
        """æ‰“å°é…ç½®ç‹€æ…‹"""
        print("âš™ï¸  DeepEval é…ç½®ç‹€æ…‹")
        print("-" * 30)
        
        validation = cls.validate_config()
        
        if validation['valid']:
            print("âœ… é…ç½®é©—è­‰é€šé")
        else:
            print("âŒ é…ç½®å­˜åœ¨å•é¡Œ:")
            for issue in validation['issues']:
                print(f"   - {issue}")
        
        if validation['warnings']:
            print("âš ï¸  è­¦å‘Š:")
            for warning in validation['warnings']:
                print(f"   - {warning}")
        
        print(f"\nğŸ“Š è©•ä¼°è¨­ç½®:")
        print(f"   - é è¨­å•é¡Œæ•¸é‡: {cls.DEFAULT_QUESTION_COUNT}")
        print(f"   - æœ€å¤§å•é¡Œæ•¸é‡: {cls.MAX_QUESTION_COUNT}")
        print(f"   - OpenAI æ¨¡å‹: {cls.OPENAI_MODEL}")
        
        print(f"\nğŸ¯ æŒ‡æ¨™é–¾å€¼:")
        for metric, threshold in cls.METRIC_THRESHOLDS.items():
            print(f"   - {metric}: {threshold}")

def main():
    """ä¸»å‡½æ•¸ - é¡¯ç¤ºé…ç½®ç‹€æ…‹"""
    DeepEvalConfig.print_config_status()

if __name__ == "__main__":
    main()