#!/usr/bin/env python3
"""
RAGAS è©•ä¼°é é¢æ¨¡çµ„
æ•´åˆ RAGAS è©•ä¼°åˆ†æåŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
import uuid

# å˜—è©¦å°å…¥ RAGAS
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy, 
        context_precision,
        context_recall,
        answer_similarity,
        answer_correctness
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False

# å°å…¥ RAGFlow å®¢æˆ¶ç«¯
try:
    from ragflow_chatbot import RAGFlowOfficialClient
    RAGFLOW_AVAILABLE = True
except ImportError:
    RAGFLOW_AVAILABLE = False

class RAGASEvaluator:
    """RAGAS è©•ä¼°å™¨é¡"""
    
    def __init__(self):
        self.client = None
        if RAGFLOW_AVAILABLE:
            self.client = RAGFlowOfficialClient()
        
        # æŒ‡æ¨™æ˜ å°„
        self.metrics_mapping = {
            'faithfulness': 'å¿ å¯¦åº¦',
            'answer_relevancy': 'ç­”æ¡ˆç›¸é—œæ€§',
            'context_precision': 'ä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦',
            'context_recall': 'ä¸Šä¸‹æ–‡å¬å›ç‡',
            'answer_similarity': 'ç­”æ¡ˆç›¸ä¼¼åº¦',
            'answer_correctness': 'ç­”æ¡ˆæ­£ç¢ºæ€§'
        }
        
        # å¯ç”¨æŒ‡æ¨™
        self.available_metrics = self.get_available_metrics()
    
    def get_available_metrics(self) -> Dict[str, Any]:
        """ç²å–å¯ç”¨çš„ RAGAS æŒ‡æ¨™"""
        if not RAGAS_AVAILABLE:
            return {}
        
        return {
            'faithfulness': faithfulness,
            'answer_relevancy': answer_relevancy,
            'context_precision': context_precision,
            'context_recall': context_recall,
            'answer_similarity': answer_similarity,
            'answer_correctness': answer_correctness
        }
    
    def generate_test_questions(self, dataset_name: str, num_questions: int) -> List[Dict[str, str]]:
        """ç”Ÿæˆæ¸¬è©¦å•é¡Œ"""
        # æ ¹æ“šæ•¸æ“šé›†åç¨±é¸æ“‡å•é¡Œæ¨¡æ¿
        if any(keyword in dataset_name.lower() for keyword in ['æ³•å¾‹', 'legal', 'æ†²æ³•', 'æ°‘æ³•', 'è¡Œæ”¿æ³•']):
            base_questions = [
                "ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ",
                "æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "åˆ‘æ³•çš„ç½ªåˆ‘æ³•å®šåŸå‰‡å¦‚ä½•ç†è§£ï¼Ÿ",
                "è¡Œæ”¿æ³•ä¸­çš„æ¯”ä¾‹åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "æ°‘äº‹è¨´è¨Ÿä¸­çš„èˆ‰è­‰è²¬ä»»å¦‚ä½•åˆ†é…ï¼Ÿ",
                "æ†²æ³•ä¿éšœå“ªäº›åŸºæœ¬æ¬Šåˆ©ï¼Ÿ",
                "ä»€éº¼æ˜¯æ³•æ²»åœ‹å®¶çš„ç‰¹å¾µï¼Ÿ",
                "è¡Œæ”¿è™•åˆ†çš„æ§‹æˆè¦ä»¶æ˜¯ä»€éº¼ï¼Ÿ",
                "æ°‘æ³•çš„æ™‚æ•ˆåˆ¶åº¦å¦‚ä½•è¦å®šï¼Ÿ",
                "åˆ‘äº‹è¨´è¨Ÿçš„åŸºæœ¬åŸå‰‡æœ‰å“ªäº›ï¼Ÿ"
            ]
        elif any(keyword in dataset_name.lower() for keyword in ['æŠ€è¡“', 'tech', 'api']):
            base_questions = [
                "ä»€éº¼æ˜¯ APIï¼Ÿ",
                "å¦‚ä½•é€²è¡Œç³»çµ±é…ç½®ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æŠ€è¡“æ¦‚å¿µï¼Ÿ",
                "å¦‚ä½•è§£æ±ºå¸¸è¦‹å•é¡Œï¼Ÿ",
                "ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹ï¼Ÿ",
                "å¦‚ä½•é€²è¡Œæ€§èƒ½å„ªåŒ–ï¼Ÿ",
                "ç³»çµ±å®‰å…¨æœ‰å“ªäº›è€ƒæ…®ï¼Ÿ",
                "ä»€éº¼æ˜¯RESTful APIï¼Ÿ",
                "å¦‚ä½•é€²è¡Œæ•¸æ“šåº«è¨­è¨ˆï¼Ÿ",
                "ä»€éº¼æ˜¯å®¹å™¨åŒ–æŠ€è¡“ï¼Ÿ"
            ]
        else:
            base_questions = [
                f"{dataset_name}åŒ…å«ä»€éº¼ä¸»è¦å…§å®¹ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æ¦‚å¿µéœ€è¦äº†è§£ï¼Ÿ",
                "é€™å€‹é ˜åŸŸçš„æœ€æ–°ç™¼å±•è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ",
                "æœ‰ä»€éº¼å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ï¼Ÿ",
                "å¦‚ä½•é–‹å§‹å­¸ç¿’é€™å€‹é ˜åŸŸï¼Ÿ",
                "å¸¸è¦‹çš„å•é¡Œæœ‰å“ªäº›ï¼Ÿ",
                "æœ€ä½³å¯¦è¸æ˜¯ä»€éº¼ï¼Ÿ",
                "æœ‰å“ªäº›å·¥å…·å’Œè³‡æºï¼Ÿ",
                "å¦‚ä½•è©•ä¼°æ•ˆæœï¼Ÿ",
                "æœªä¾†ç™¼å±•æ–¹å‘å¦‚ä½•ï¼Ÿ"
            ]
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        questions = []
        for i in range(min(num_questions, len(base_questions))):
            questions.append({
                "id": f"q_{i+1}",
                "question": base_questions[i],
                "expected_answer": "",  # å¾…å¡«å¯«
                "tags": [dataset_name]
            })
        
        return questions
    
    def evaluate_with_ragas(self, test_cases: List[Dict], selected_metrics: List[str]) -> Dict:
        """ä½¿ç”¨ RAGAS é€²è¡Œè©•ä¼°"""
        if not RAGAS_AVAILABLE:
            return {
                'success': False,
                'message': 'RAGAS ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£'
            }
        
        try:
            # æº–å‚™æ•¸æ“š
            data = {
                'question': [case['question'] for case in test_cases],
                'answer': [case['answer'] for case in test_cases],
                'contexts': [case['contexts'] for case in test_cases],
                'ground_truth': [case.get('expected_answer', '') for case in test_cases]
            }
            
            dataset = Dataset.from_dict(data)
            
            # é¸æ“‡è©•ä¼°æŒ‡æ¨™
            metrics = []
            for metric_name in selected_metrics:
                if metric_name in self.available_metrics:
                    metrics.append(self.available_metrics[metric_name])
            
            if not metrics:
                return {
                    'success': False,
                    'message': 'æ²’æœ‰é¸æ“‡æœ‰æ•ˆçš„è©•ä¼°æŒ‡æ¨™'
                }
            
            # åŸ·è¡Œè©•ä¼°
            evaluation_result = evaluate(dataset, metrics=metrics)
            
            # å°‡ EvaluationResult è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„å­—å…¸
            results_dict = {}
            
            # RAGAS EvaluationResult æ­£ç¢ºçš„è¨ªå•æ–¹å¼
            if hasattr(evaluation_result, '__getitem__'):
                # EvaluationResult æ”¯æ´ç´¢å¼•è¨ªå•ï¼Œä½†è¿”å›çš„æ˜¯åˆ—è¡¨
                for metric_name in selected_metrics:
                    try:
                        score_list = evaluation_result[metric_name]
                        if isinstance(score_list, list) and len(score_list) > 0:
                            score = score_list[0]  # å–ç¬¬ä¸€å€‹å€¼
                            if hasattr(score, 'item'):  # numpy scalar
                                score_value = float(score.item())
                            else:
                                score_value = float(score)
                            
                            # è™•ç† NaN å’Œç„¡ç©·å¤§å€¼
                            import math
                            if math.isnan(score_value) or math.isinf(score_value):
                                results_dict[metric_name] = 0.0
                            else:
                                results_dict[metric_name] = score_value
                        else:
                            results_dict[metric_name] = 0.0
                    except (KeyError, IndexError, ValueError):
                        results_dict[metric_name] = 0.0
            elif hasattr(evaluation_result, 'to_pandas'):
                # ä½¿ç”¨ to_pandas æ–¹æ³•ç²å–çµæœ
                try:
                    df = evaluation_result.to_pandas()
                    for metric_name in selected_metrics:
                        if metric_name in df.columns:
                            score = df[metric_name].iloc[0] if len(df) > 0 else 0.0
                            results_dict[metric_name] = float(score)
                        else:
                            results_dict[metric_name] = 0.0
                except Exception as e:
                    st.warning(f"to_pandas æ–¹æ³•å¤±æ•—: {e}")
                    results_dict = {metric: 0.0 for metric in selected_metrics}
            else:
                # å‚™é¸æ–¹æ¡ˆï¼šå°æ–¼ä¸è­˜åˆ¥çš„é¡å‹
                st.warning(f"ä¸è­˜åˆ¥çš„è©•ä¼°çµæœé¡å‹: {type(evaluation_result)}")
                results_dict = {metric: 0.0 for metric in selected_metrics}
            
            return {
                'success': True,
                'data': results_dict,
                'metrics_used': selected_metrics
            }
            
        except Exception as e:
            # åªåœ¨éœ€è¦æ™‚é¡¯ç¤ºè©³ç´°éŒ¯èª¤
            if 'debug_mode' in st.session_state and st.session_state.debug_mode:
                st.error(f"è©³ç´°éŒ¯èª¤ä¿¡æ¯: {str(e)}")
                st.error(f"éŒ¯èª¤é¡å‹: {type(e).__name__}")
                import traceback
                st.error(f"å †æ£§è¿½è¹¤: {traceback.format_exc()[:500]}...")
            return {
                'success': False,
                'message': f'è©•ä¼°å¤±æ•—: {str(e)}'
            }
    
    def save_evaluation_results(self, results: Dict, test_cases: List[Dict]) -> str:
        """ä¿å­˜è©•ä¼°çµæœ"""
        try:
            # å‰µå»ºçµæœç›®éŒ„
            os.makedirs('data/evaluations', exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"evaluation_{timestamp}.json"
            filepath = f"data/evaluations/{filename}"
            
            # æº–å‚™ä¿å­˜æ•¸æ“š
            save_data = {
                "evaluation_id": str(uuid.uuid4()),
                "timestamp": datetime.now().isoformat(),
                "dataset_id": st.session_state.current_dataset.get('id') if st.session_state.current_dataset else None,
                "dataset_name": st.session_state.current_dataset.get('name') if st.session_state.current_dataset else None,
                "test_cases_count": len(test_cases),
                "results": results,
                "test_cases": test_cases
            }
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            return filepath
            
        except Exception as e:
            st.error(f"ä¿å­˜è©•ä¼°çµæœå¤±æ•—: {e}")
            if 'debug_mode' in st.session_state and st.session_state.debug_mode:
                import traceback
                st.error(traceback.format_exc())
            return None

def show_evaluation_page():
    """é¡¯ç¤ºè©•ä¼°é é¢"""
    st.markdown("## ğŸ“ RAGAS è©•ä¼°")
    st.markdown("ä½¿ç”¨ RAGAS æ¡†æ¶å° RAG ç³»çµ±é€²è¡Œå¤šç¶­åº¦è©•ä¼°åˆ†æ")
    
    # æª¢æŸ¥ä¾è³´
    if not RAGAS_AVAILABLE:
        st.error("âŒ RAGAS ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£")
        st.markdown("""
        ### å®‰è£èªªæ˜
        ```bash
        pip install ragas datasets
        ```
        """)
        return
    
    if not RAGFLOW_AVAILABLE:
        st.error("âŒ RAGFlow å®¢æˆ¶ç«¯ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥é…ç½®")
        return
    
    # åˆå§‹åŒ–è©•ä¼°å™¨
    if 'ragas_evaluator' not in st.session_state:
        st.session_state.ragas_evaluator = RAGASEvaluator()
    
    evaluator = st.session_state.ragas_evaluator
    
    # è©•ä¼°æºé¸æ“‡
    st.markdown("### ğŸ“š è©•ä¼°æ•¸æ“šä¾†æº")
    
    data_source = st.radio(
        "é¸æ“‡æ•¸æ“šä¾†æº",
        ["èŠå¤©è¨˜éŒ„", "æ‰‹å‹•å‰µå»º", "ä¸Šå‚³æ–‡ä»¶"],
        horizontal=True
    )
    
    test_cases = []
    
    if data_source == "èŠå¤©è¨˜éŒ„":
        if 'chat_for_evaluation' in st.session_state and st.session_state.chat_for_evaluation:
            st.success(f"âœ… æ‰¾åˆ° {len(st.session_state.chat_for_evaluation)} å€‹èŠå¤©è¨˜éŒ„å¯ç”¨æ–¼è©•ä¼°")
            
            # é¡¯ç¤ºèŠå¤©è¨˜éŒ„é è¦½
            with st.expander("ğŸ“‹ èŠå¤©è¨˜éŒ„é è¦½"):
                for i, case in enumerate(st.session_state.chat_for_evaluation[:5]):
                    st.markdown(f"**å•é¡Œ {i+1}:** {case['question']}")
                    st.markdown(f"**å›ç­”:** {case['answer'][:100]}...")
                    st.markdown("---")
            
            test_cases = st.session_state.chat_for_evaluation
        else:
            st.warning("âš ï¸ æ²’æœ‰å¯ç”¨çš„èŠå¤©è¨˜éŒ„ï¼Œè«‹å…ˆé€²è¡ŒèŠå¤©å°è©±")
    
    elif data_source == "æ‰‹å‹•å‰µå»º":
        st.markdown("#### ğŸ› ï¸ æ‰‹å‹•å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹")
        
        if st.session_state.current_dataset:
            dataset_name = st.session_state.current_dataset.get('name', 'Unknown')
            num_questions = st.slider("ç”Ÿæˆå•é¡Œæ•¸é‡", 1, 20, 5)
            
            if st.button("ğŸ¯ ç”Ÿæˆæ¸¬è©¦å•é¡Œ"):
                generated_questions = evaluator.generate_test_questions(dataset_name, num_questions)
                st.session_state.generated_questions = generated_questions
                st.success(f"âœ… å·²ç”Ÿæˆ {len(generated_questions)} å€‹æ¸¬è©¦å•é¡Œ")
            
            if 'generated_questions' in st.session_state:
                st.markdown("#### ğŸ“ ç·¨è¼¯æ¸¬è©¦å•é¡Œ")
                for i, q in enumerate(st.session_state.generated_questions):
                    with st.expander(f"å•é¡Œ {i+1}: {q['question'][:50]}..."):
                        q['question'] = st.text_area(f"å•é¡Œ {i+1}", q['question'], key=f"q_{i}")
                        q['expected_answer'] = st.text_area(f"æœŸæœ›ç­”æ¡ˆ {i+1}", q.get('expected_answer', ''), key=f"a_{i}")
                
                if st.button("ğŸš€ ç²å– RAG å›ç­”ä¸¦è©•ä¼°"):
                    # é€™è£¡éœ€è¦èª¿ç”¨ RAGFlow ç²å–å›ç­”
                    with st.spinner("æ­£åœ¨ç²å– RAG ç³»çµ±å›ç­”..."):
                        # æ¨¡æ“¬ç²å–å›ç­”çš„éç¨‹
                        for q in st.session_state.generated_questions:
                            # å¯¦éš›å¯¦ç¾ä¸­æ‡‰è©²èª¿ç”¨ RAGFlow API
                            q['answer'] = f"é€™æ˜¯é‡å°å•é¡Œ '{q['question']}' çš„æ¨¡æ“¬å›ç­”"
                            q['contexts'] = [f"ç›¸é—œä¸Šä¸‹æ–‡ä¿¡æ¯ for {q['question']}"]
                        
                        test_cases = st.session_state.generated_questions
                        st.success("âœ… å·²ç²å–æ‰€æœ‰å•é¡Œçš„å›ç­”")
        else:
            st.warning("âš ï¸ è«‹å…ˆé¸æ“‡æ•¸æ“šé›†")
    
    elif data_source == "ä¸Šå‚³æ–‡ä»¶":
        uploaded_file = st.file_uploader("ä¸Šå‚³æ¸¬è©¦æ¡ˆä¾‹æ–‡ä»¶ (JSONæ ¼å¼)", type=['json'])
        if uploaded_file:
            try:
                test_cases = json.loads(uploaded_file.read().decode('utf-8'))
                st.success(f"âœ… å·²ä¸Šå‚³ {len(test_cases)} å€‹æ¸¬è©¦æ¡ˆä¾‹")
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶è§£æå¤±æ•—: {e}")
    
    # è©•ä¼°é…ç½®
    if test_cases:
        st.markdown("---")
        st.markdown("### âš™ï¸ è©•ä¼°é…ç½®")
        
        # æŒ‡æ¨™é¸æ“‡
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ğŸ“Š é¸æ“‡è©•ä¼°æŒ‡æ¨™")
            selected_metrics = []
            
            for metric_key, metric_name in evaluator.metrics_mapping.items():
                if st.checkbox(metric_name, key=f"metric_{metric_key}"):
                    selected_metrics.append(metric_key)
        
        with col2:
            st.markdown("#### ğŸ”§ è©•ä¼°åƒæ•¸")
            
            # OpenAI API Key æª¢æŸ¥
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                st.success("âœ… OpenAI API Key å·²é…ç½®")
            else:
                st.warning("âš ï¸ æœªæª¢æ¸¬åˆ° OpenAI API Key")
                st.markdown("éƒ¨åˆ† RAGAS æŒ‡æ¨™éœ€è¦ OpenAI API")
            
            # è©•ä¼°é–¾å€¼è¨­ç½®å’Œèª¿è©¦æ¨¡å¼
            col1, col2 = st.columns([3, 1])
            with col1:
                threshold = st.slider("è©•ä¼°é–¾å€¼", 0.0, 1.0, 0.7, 0.1)
                st.info(f"ä½æ–¼ {threshold} çš„æŒ‡æ¨™å°‡è¢«æ¨™è¨˜ç‚ºéœ€è¦æ”¹é€²")
            with col2:
                debug_mode = st.checkbox("èª¿è©¦æ¨¡å¼", help="é¡¯ç¤ºè©³ç´°éŒ¯èª¤ä¿¡æ¯")
                st.session_state.debug_mode = debug_mode
        
        # é–‹å§‹è©•ä¼°
        if selected_metrics and st.button("ğŸš€ é–‹å§‹ RAGAS è©•ä¼°", type="primary"):
            with st.spinner("æ­£åœ¨åŸ·è¡Œ RAGAS è©•ä¼°..."):
                # é¡¯ç¤ºé€²åº¦æ¢
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # åŸ·è¡Œè©•ä¼°
                try:
                    status_text.text("æº–å‚™è©•ä¼°æ•¸æ“š...")
                    progress_bar.progress(20)
                    
                    status_text.text("åŸ·è¡Œ RAGAS è©•ä¼°...")
                    progress_bar.progress(60)
                    
                    results = evaluator.evaluate_with_ragas(test_cases, selected_metrics)
                    progress_bar.progress(80)
                    
                    if results['success']:
                        status_text.text("ä¿å­˜è©•ä¼°çµæœ...")
                        
                        # ä¿å­˜çµæœ
                        filepath = evaluator.save_evaluation_results(results['data'], test_cases)
                        progress_bar.progress(100)
                        
                        # å°‡çµæœæ·»åŠ åˆ° session state
                        if 'evaluation_results' not in st.session_state:
                            st.session_state.evaluation_results = []
                        
                        evaluation_record = {
                            'timestamp': datetime.now().isoformat(),
                            'results': results['data'],
                            'test_cases_count': len(test_cases),
                            'metrics_used': selected_metrics,
                            'filepath': filepath
                        }
                        st.session_state.evaluation_results.append(evaluation_record)
                        
                        st.success("âœ… RAGAS è©•ä¼°å®Œæˆï¼")
                        status_text.empty()
                        
                        # é¡¯ç¤ºè©•ä¼°çµæœ
                        st.markdown("---")
                        st.markdown("### ğŸ“Š è©•ä¼°çµæœ")
                        
                        # çµæœæ¦‚è¦½
                        results_data = results['data']
                        
                        # å‰µå»ºæŒ‡æ¨™å¡ç‰‡
                        cols = st.columns(len(selected_metrics))
                        for i, metric in enumerate(selected_metrics):
                            with cols[i]:
                                if metric in results_data:
                                    score = results_data[metric]
                                    metric_name = evaluator.metrics_mapping[metric]
                                    
                                    # æ ¹æ“šåˆ†æ•¸æ±ºå®šé¡è‰²
                                    if score >= 0.8:
                                        color = "ğŸŸ¢"
                                    elif score >= threshold:
                                        color = "ğŸŸ¡"
                                    else:
                                        color = "ğŸ”´"
                                    
                                    st.metric(
                                        f"{color} {metric_name}",
                                        f"{score:.3f}",
                                        f"{'âœ…' if score >= threshold else 'âš ï¸'}"
                                    )
                        
                        # è©³ç´°çµæœè¡¨æ ¼
                        st.markdown("#### ğŸ“‹ è©³ç´°çµæœ")
                        results_df = pd.DataFrame([{
                            'æŒ‡æ¨™': evaluator.metrics_mapping[metric],
                            'åˆ†æ•¸': f"{results_data[metric]:.3f}",
                            'ç‹€æ…‹': 'âœ… è‰¯å¥½' if results_data[metric] >= threshold else 'âš ï¸ éœ€æ”¹é€²'
                        } for metric in selected_metrics if metric in results_data])
                        
                        st.dataframe(results_df, use_container_width=True)
                        
                        # çµæœä¸‹è¼‰
                        if filepath:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                result_json = f.read()
                            
                            st.download_button(
                                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´è©•ä¼°çµæœ",
                                data=result_json,
                                file_name=f"ragas_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                                mime="application/json"
                            )
                    
                    else:
                        st.error(f"âŒ è©•ä¼°å¤±æ•—: {results['message']}")
                        progress_bar.empty()
                        status_text.empty()
                
                except Exception as e:
                    st.error(f"âŒ è©•ä¼°éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                    progress_bar.empty()
                    status_text.empty()
        
        elif not selected_metrics:
            st.warning("âš ï¸ è«‹è‡³å°‘é¸æ“‡ä¸€å€‹è©•ä¼°æŒ‡æ¨™")
    
    else:
        st.info("ğŸ’¡ è«‹é¸æ“‡æˆ–å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹ä»¥é–‹å§‹è©•ä¼°")