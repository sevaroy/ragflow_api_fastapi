#!/usr/bin/env python3
"""
DeepEval è©•ä¼°çµæœå„€è¡¨æ¿
ä½¿ç”¨ Streamlit å‰µå»ºå°ˆæ¥­çš„è©•ä¼°æ•¸æ“šè¦–è¦ºåŒ–ç•Œé¢
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
from datetime import datetime
from typing import Dict, List, Any
import glob

# è¨­ç½®é é¢é…ç½®
st.set_page_config(
    page_title="DeepEval è©•ä¼°å„€è¡¨æ¿",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

class DeepEvalDashboard:
    """DeepEval å„€è¡¨æ¿é¡"""
    
    def __init__(self):
        self.evaluation_data = None
        self.metrics_data = None
        self.summary_stats = None
    
    def load_evaluation_results(self, file_path: str) -> bool:
        """è¼‰å…¥è©•ä¼°çµæœ JSON æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                self.evaluation_data = json.load(f)
            
            # è™•ç†æ•¸æ“šæ ¼å¼
            if isinstance(self.evaluation_data, list):
                # å¦‚æœæ˜¯çµæœåˆ—è¡¨
                self.process_results_list()
            elif isinstance(self.evaluation_data, dict):
                # å¦‚æœæ˜¯åŒ…å«æ‘˜è¦çš„å­—å…¸
                self.process_results_dict()
            
            return True
            
        except Exception as e:
            st.error(f"è¼‰å…¥è©•ä¼°çµæœå¤±æ•—: {e}")
            return False
    
    def process_results_list(self):
        """è™•ç†çµæœåˆ—è¡¨æ ¼å¼"""
        results_df = []
        
        for i, result in enumerate(self.evaluation_data):
            row = {
                'test_id': result.get('test_case_id', f'test_{i+1}'),
                'question': result.get('question', ''),
                'actual_output': result.get('actual_output', ''),
                'expected_output': result.get('expected_output', ''),
                'overall_score': result.get('overall_score', 0),
                'passed': result.get('passed', False)
            }
            
            # æ·»åŠ å„é …æŒ‡æ¨™åˆ†æ•¸
            metrics_scores = result.get('metrics_scores', {})
            for metric, score in metrics_scores.items():
                row[metric] = score
            
            results_df.append(row)
        
        self.metrics_data = pd.DataFrame(results_df)
        self.calculate_summary_stats()
    
    def process_results_dict(self):
        """è™•ç†åŒ…å«æ‘˜è¦çš„å­—å…¸æ ¼å¼"""
        if 'dataset_results' in self.evaluation_data:
            # è™•ç†å¤šæ•¸æ“šé›†çµæœ
            all_results = []
            for dataset_result in self.evaluation_data['dataset_results']:
                # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›æ•¸æ“šçµæ§‹èª¿æ•´
                all_results.extend(dataset_result.get('results', []))
            self.evaluation_data = all_results
            self.process_results_list()
        else:
            # å–®ä¸€çµæœé›†
            self.process_results_list()
    
    def calculate_summary_stats(self):
        """è¨ˆç®—æ‘˜è¦çµ±è¨ˆ"""
        if self.metrics_data is None or self.metrics_data.empty:
            return
        
        total_cases = len(self.metrics_data)
        passed_cases = self.metrics_data['passed'].sum()
        pass_rate = passed_cases / total_cases * 100 if total_cases > 0 else 0
        avg_score = self.metrics_data['overall_score'].mean()
        
        # è¨ˆç®—å„æŒ‡æ¨™çµ±è¨ˆ
        metric_columns = [col for col in self.metrics_data.columns 
                         if col not in ['test_id', 'question', 'actual_output', 
                                       'expected_output', 'overall_score', 'passed']]
        
        metric_stats = {}
        for metric in metric_columns:
            if metric in self.metrics_data.columns:
                metric_stats[metric] = {
                    'mean': self.metrics_data[metric].mean(),
                    'min': self.metrics_data[metric].min(),
                    'max': self.metrics_data[metric].max(),
                    'std': self.metrics_data[metric].std()
                }
        
        self.summary_stats = {
            'total_cases': total_cases,
            'passed_cases': passed_cases,
            'pass_rate': pass_rate,
            'avg_score': avg_score,
            'metric_stats': metric_stats
        }
    
    def render_header(self):
        """æ¸²æŸ“é é¢æ¨™é¡Œ"""
        st.title("ğŸ“Š DeepEval è©•ä¼°å„€è¡¨æ¿")
        st.markdown("---")
        
        # é¡¯ç¤ºè¼‰å…¥çš„æ–‡ä»¶ä¿¡æ¯
        if hasattr(st.session_state, 'loaded_file'):
            st.info(f"ğŸ“ ç•¶å‰è¼‰å…¥æ–‡ä»¶: {st.session_state.loaded_file}")
    
    def render_summary_metrics(self):
        """æ¸²æŸ“æ‘˜è¦æŒ‡æ¨™"""
        if not self.summary_stats:
            st.warning("æ²’æœ‰å¯ç”¨çš„æ‘˜è¦çµ±è¨ˆæ•¸æ“š")
            return
        
        st.subheader("ğŸ“ˆ æ•´é«”è¡¨ç¾æ‘˜è¦")
        
        # å‰µå»ºå››åˆ—å¸ƒå±€
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="ç¸½æ¸¬è©¦æ¡ˆä¾‹",
                value=self.summary_stats['total_cases']
            )
        
        with col2:
            st.metric(
                label="é€šéæ¡ˆä¾‹",
                value=self.summary_stats['passed_cases'],
                delta=f"{self.summary_stats['pass_rate']:.1f}%"
            )
        
        with col3:
            st.metric(
                label="é€šéç‡",
                value=f"{self.summary_stats['pass_rate']:.1f}%",
                delta="ç›®æ¨™: 80%" if self.summary_stats['pass_rate'] >= 80 else "éœ€è¦æ”¹é€²"
            )
        
        with col4:
            st.metric(
                label="å¹³å‡åˆ†æ•¸",
                value=f"{self.summary_stats['avg_score']:.3f}",
                delta="è‰¯å¥½" if self.summary_stats['avg_score'] >= 0.7 else "éœ€è¦æ”¹é€²"
            )
    
    def render_metrics_overview(self):
        """æ¸²æŸ“æŒ‡æ¨™æ¦‚è¦½"""
        if not self.summary_stats or not self.summary_stats['metric_stats']:
            return
        
        st.subheader("ğŸ¯ è©•ä¼°æŒ‡æ¨™è©³æƒ…")
        
        # å‰µå»ºæŒ‡æ¨™å°æ¯”åœ–
        metrics_df = pd.DataFrame(self.summary_stats['metric_stats']).T
        metrics_df = metrics_df.reset_index()
        metrics_df.columns = ['metric', 'mean', 'min', 'max', 'std']
        
        # é›·é”åœ–
        fig_radar = go.Figure()
        
        fig_radar.add_trace(go.Scatterpolar(
            r=metrics_df['mean'].tolist(),
            theta=metrics_df['metric'].tolist(),
            fill='toself',
            name='å¹³å‡åˆ†æ•¸',
            line_color='rgb(0, 123, 255)'
        ))
        
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )),
            showlegend=True,
            title="è©•ä¼°æŒ‡æ¨™é›·é”åœ–"
        )
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.plotly_chart(fig_radar, use_container_width=True)
        
        with col2:
            # æŒ‡æ¨™è©³ç´°è¡¨æ ¼
            st.subheader("æŒ‡æ¨™çµ±è¨ˆè¡¨")
            
            # æ ¼å¼åŒ–æ•¸æ“š
            display_df = metrics_df.copy()
            for col in ['mean', 'min', 'max', 'std']:
                display_df[col] = display_df[col].round(3)
            
            st.dataframe(
                display_df,
                column_config={
                    "metric": "æŒ‡æ¨™",
                    "mean": "å¹³å‡å€¼",
                    "min": "æœ€å°å€¼", 
                    "max": "æœ€å¤§å€¼",
                    "std": "æ¨™æº–å·®"
                },
                hide_index=True,
                use_container_width=True
            )
    
    def render_detailed_results(self):
        """æ¸²æŸ“è©³ç´°çµæœ"""
        if self.metrics_data is None or self.metrics_data.empty:
            return
        
        st.subheader("ğŸ“‹ è©³ç´°è©•ä¼°çµæœ")
        
        # ç¯©é¸é¸é …
        col1, col2, col3 = st.columns(3)
        
        with col1:
            show_passed_only = st.checkbox("åªé¡¯ç¤ºé€šéçš„æ¡ˆä¾‹")
        
        with col2:
            show_failed_only = st.checkbox("åªé¡¯ç¤ºå¤±æ•—çš„æ¡ˆä¾‹")
        
        with col3:
            min_score = st.slider("æœ€ä½åˆ†æ•¸ç¯©é¸", 0.0, 1.0, 0.0, 0.1)
        
        # æ‡‰ç”¨ç¯©é¸
        filtered_data = self.metrics_data.copy()
        
        if show_passed_only:
            filtered_data = filtered_data[filtered_data['passed'] == True]
        elif show_failed_only:
            filtered_data = filtered_data[filtered_data['passed'] == False]
        
        filtered_data = filtered_data[filtered_data['overall_score'] >= min_score]
        
        # é¡¯ç¤ºç¯©é¸å¾Œçš„çµæœ
        st.write(f"é¡¯ç¤º {len(filtered_data)} / {len(self.metrics_data)} å€‹çµæœ")
        
        # è©³ç´°çµæœå±•ç¤º
        for idx, row in filtered_data.iterrows():
            with st.expander(
                f"{'âœ…' if row['passed'] else 'âŒ'} {row['test_id']} - åˆ†æ•¸: {row['overall_score']:.3f}",
                expanded=False
            ):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write("**å•é¡Œ:**")
                    st.write(row['question'])
                    
                    st.write("**å¯¦éš›å›ç­”:**")
                    st.write(row['actual_output'][:300] + "..." if len(row['actual_output']) > 300 else row['actual_output'])
                    
                    if row['expected_output']:
                        st.write("**æœŸæœ›å›ç­”:**")
                        st.write(row['expected_output'][:300] + "..." if len(row['expected_output']) > 300 else row['expected_output'])
                
                with col2:
                    st.write("**æŒ‡æ¨™åˆ†æ•¸:**")
                    
                    # é¡¯ç¤ºå„é …æŒ‡æ¨™
                    metric_columns = [col for col in row.index 
                                    if col not in ['test_id', 'question', 'actual_output', 
                                                  'expected_output', 'overall_score', 'passed']]
                    
                    for metric in metric_columns:
                        if pd.notna(row[metric]):
                            score = row[metric]
                            color = "green" if score >= 0.7 else "orange" if score >= 0.5 else "red"
                            st.markdown(f"**{metric}**: <span style='color:{color}'>{score:.3f}</span>", 
                                      unsafe_allow_html=True)
    
    def render_charts(self):
        """æ¸²æŸ“åœ–è¡¨åˆ†æ"""
        if self.metrics_data is None or self.metrics_data.empty:
            return
        
        st.subheader("ğŸ“Š æ•¸æ“šåˆ†æåœ–è¡¨")
        
        # åˆ†æ•¸åˆ†å¸ƒç›´æ–¹åœ–
        fig_hist = px.histogram(
            self.metrics_data, 
            x='overall_score',
            nbins=20,
            title="æ•´é«”åˆ†æ•¸åˆ†å¸ƒ",
            labels={'overall_score': 'æ•´é«”åˆ†æ•¸', 'count': 'æ¡ˆä¾‹æ•¸é‡'}
        )
        
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # é€šé/å¤±æ•—æ¯”ä¾‹é¤…åœ–
        pass_fail_counts = self.metrics_data['passed'].value_counts()
        
        fig_pie = px.pie(
            values=pass_fail_counts.values,
            names=['å¤±æ•—', 'é€šé'] if False in pass_fail_counts.index else ['é€šé'],
            title="é€šé/å¤±æ•—æ¯”ä¾‹",
            color_discrete_map={True: 'green', False: 'red'}
        )
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.plotly_chart(fig_pie, use_container_width=True)
        
        with col2:
            # æŒ‡æ¨™ç›¸é—œæ€§ç†±åŠ›åœ–
            metric_columns = [col for col in self.metrics_data.columns 
                            if col not in ['test_id', 'question', 'actual_output', 
                                          'expected_output', 'passed']]
            
            if len(metric_columns) > 1:
                corr_matrix = self.metrics_data[metric_columns].corr()
                
                fig_heatmap = px.imshow(
                    corr_matrix,
                    title="æŒ‡æ¨™ç›¸é—œæ€§ç†±åŠ›åœ–",
                    color_continuous_scale="RdBu",
                    aspect="auto"
                )
                
                st.plotly_chart(fig_heatmap, use_container_width=True)
    
    def render_export_options(self):
        """æ¸²æŸ“å°å‡ºé¸é …"""
        st.subheader("ğŸ’¾ å°å‡ºé¸é …")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("å°å‡º CSV"):
                if self.metrics_data is not None:
                    csv = self.metrics_data.to_csv(index=False)
                    st.download_button(
                        label="ä¸‹è¼‰ CSV æ–‡ä»¶",
                        data=csv,
                        file_name=f"deepeval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
        
        with col2:
            if st.button("å°å‡ºæ‘˜è¦å ±å‘Š"):
                if self.summary_stats:
                    report = self.generate_summary_report()
                    st.download_button(
                        label="ä¸‹è¼‰æ‘˜è¦å ±å‘Š",
                        data=report,
                        file_name=f"deepeval_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                        mime="text/markdown"
                    )
        
        with col3:
            if st.button("å°å‡º JSON"):
                if self.evaluation_data:
                    json_str = json.dumps(self.evaluation_data, ensure_ascii=False, indent=2)
                    st.download_button(
                        label="ä¸‹è¼‰ JSON æ–‡ä»¶",
                        data=json_str,
                        file_name=f"deepeval_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
        if not self.summary_stats:
            return "æ²’æœ‰å¯ç”¨çš„çµ±è¨ˆæ•¸æ“š"
        
        report = f"""# DeepEval è©•ä¼°æ‘˜è¦å ±å‘Š

## ğŸ“Š æ•´é«”çµ±è¨ˆ
- **ç¸½æ¸¬è©¦æ¡ˆä¾‹**: {self.summary_stats['total_cases']}
- **é€šéæ¡ˆä¾‹**: {self.summary_stats['passed_cases']}
- **é€šéç‡**: {self.summary_stats['pass_rate']:.1f}%
- **å¹³å‡åˆ†æ•¸**: {self.summary_stats['avg_score']:.3f}

## ğŸ“‹ æŒ‡æ¨™è©³æƒ…
"""
        
        for metric, stats in self.summary_stats['metric_stats'].items():
            report += f"""
### {metric}
- å¹³å‡å€¼: {stats['mean']:.3f}
- æœ€å°å€¼: {stats['min']:.3f}
- æœ€å¤§å€¼: {stats['max']:.3f}
- æ¨™æº–å·®: {stats['std']:.3f}
"""
        
        report += f"""
## ğŸ’¡ å»ºè­°
- {'âœ… ç³»çµ±è¡¨ç¾è‰¯å¥½' if self.summary_stats['pass_rate'] >= 80 else 'âš ï¸ éœ€è¦æ”¹é€²ç³»çµ±æ€§èƒ½'}
- {'âœ… å¹³å‡åˆ†æ•¸é”æ¨™' if self.summary_stats['avg_score'] >= 0.7 else 'âš ï¸ éœ€è¦æå‡å›ç­”å“è³ª'}

---
å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return report

def main():
    """ä¸»å‡½æ•¸"""
    dashboard = DeepEvalDashboard()
    
    # æ¸²æŸ“æ¨™é¡Œ
    dashboard.render_header()
    
    # å´é‚Šæ¬„ - æ–‡ä»¶è¼‰å…¥
    st.sidebar.title("ğŸ“ æ•¸æ“šè¼‰å…¥")
    
    # æŸ¥æ‰¾å¯ç”¨çš„è©•ä¼°çµæœæ–‡ä»¶
    json_files = glob.glob("*.json") + glob.glob("evaluation_*.json")
    
    if json_files:
        st.sidebar.subheader("é¸æ“‡ç¾æœ‰æ–‡ä»¶")
        selected_file = st.sidebar.selectbox(
            "å¯ç”¨çš„è©•ä¼°çµæœæ–‡ä»¶:",
            [""] + json_files
        )
        
        if selected_file and st.sidebar.button("è¼‰å…¥é¸ä¸­æ–‡ä»¶"):
            if dashboard.load_evaluation_results(selected_file):
                st.session_state.loaded_file = selected_file
                st.success(f"âœ… æˆåŠŸè¼‰å…¥: {selected_file}")
                st.rerun()
    
    # æ–‡ä»¶ä¸Šå‚³
    st.sidebar.subheader("ä¸Šå‚³æ–°æ–‡ä»¶")
    uploaded_file = st.sidebar.file_uploader(
        "ä¸Šå‚³ DeepEval çµæœ JSON æ–‡ä»¶",
        type=['json'],
        help="ä¸Šå‚³ç”± DeepEval ç”Ÿæˆçš„è©•ä¼°çµæœ JSON æ–‡ä»¶"
    )
    
    if uploaded_file is not None:
        # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶
        file_path = f"uploaded_{uploaded_file.name}"
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        if dashboard.load_evaluation_results(file_path):
            st.session_state.loaded_file = file_path
            st.success(f"âœ… æˆåŠŸè¼‰å…¥ä¸Šå‚³æ–‡ä»¶: {uploaded_file.name}")
            st.rerun()
    
    # å¦‚æœæœ‰æ•¸æ“šï¼Œé¡¯ç¤ºå„€è¡¨æ¿
    if dashboard.summary_stats:
        # æ¸²æŸ“å„å€‹éƒ¨åˆ†
        dashboard.render_summary_metrics()
        st.markdown("---")
        
        dashboard.render_metrics_overview()
        st.markdown("---")
        
        dashboard.render_charts()
        st.markdown("---")
        
        dashboard.render_detailed_results()
        st.markdown("---")
        
        dashboard.render_export_options()
    
    else:
        # æ²’æœ‰æ•¸æ“šæ™‚çš„æç¤º
        st.info("ğŸ‘† è«‹åœ¨å´é‚Šæ¬„è¼‰å…¥ DeepEval è©•ä¼°çµæœæ–‡ä»¶ä¾†é–‹å§‹åˆ†æ")
        
        # é¡¯ç¤ºç¤ºä¾‹æ•¸æ“šæ ¼å¼
        st.subheader("ğŸ“‹ æ”¯æ´çš„æ•¸æ“šæ ¼å¼")
        
        example_data = {
            "test_case_id": "test_1",
            "question": "ä»€éº¼æ˜¯æ†²æ³•ç¬¬7æ¢ï¼Ÿ",
            "actual_output": "æ†²æ³•ç¬¬7æ¢è¦å®šå¹³ç­‰åŸå‰‡...",
            "expected_output": "æ†²æ³•ç¬¬7æ¢è¦å®šä¸­è¯æ°‘åœ‹äººæ°‘åœ¨æ³•å¾‹ä¸Šä¸€å¾‹å¹³ç­‰...",
            "overall_score": 0.756,
            "passed": True,
            "metrics_scores": {
                "answer_relevancy": 0.823,
                "faithfulness": 0.789,
                "hallucination": 0.234
            }
        }
        
        st.json(example_data)
        
        st.markdown("""
        **æ”¯æ´çš„æ–‡ä»¶æ ¼å¼:**
        - å–®ä¸€è©•ä¼°çµæœçš„ JSON æ•¸çµ„
        - åŒ…å«æ‘˜è¦çµ±è¨ˆçš„ JSON å°è±¡
        - ç”± `deepeval_integration.py` ç”Ÿæˆçš„çµæœæ–‡ä»¶
        """)

if __name__ == "__main__":
    main()