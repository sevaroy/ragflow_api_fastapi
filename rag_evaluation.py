#!/usr/bin/env python3
"""
ä¿®å¾©ç‰ˆ RAG è©•ä¼°ç³»çµ±
å¾¹åº•è§£æ±ºå­—ç¬¦ä¸²åˆ‡ç‰‡å•é¡Œ
"""

import streamlit as st
import os
import json
from datetime import datetime
from typing import Dict, List, Any, Optional

# åªåœ¨æœ‰ OpenAI API Key æ™‚æ‰å°å…¥ DeepEval
try:
    openai_key = os.getenv("OPENAI_API_KEY")
    if openai_key:
        from deepeval.metrics import (
            AnswerRelevancyMetric,
            FaithfulnessMetric,
            ContextualPrecisionMetric,
            ContextualRecallMetric,
            HallucinationMetric,
            BiasMetric
        )
        from deepeval.test_case import LLMTestCase
        DEEPEVAL_AVAILABLE = True
    else:
        DEEPEVAL_AVAILABLE = False
except ImportError:
    DEEPEVAL_AVAILABLE = False

# å°å…¥æœ¬åœ°æ¨¡çµ„
from ragflow_chatbot import RAGFlowOfficialClient, RAGFlowChatbot

class SimpleRAGEvaluator:
    """ç°¡åŒ–ç‰ˆ RAG è©•ä¼°å™¨"""
    
    def __init__(self):
        self.client = RAGFlowOfficialClient()
        self.chatbot = None
        self.openai_key = os.getenv("OPENAI_API_KEY")
        
        # åˆå§‹åŒ–è©•ä¼°æŒ‡æ¨™
        self.metrics = {}
        if DEEPEVAL_AVAILABLE and self.openai_key:
            try:
                self.metrics = {
                    'answer_relevancy': AnswerRelevancyMetric(threshold=0.7),
                    'faithfulness': FaithfulnessMetric(threshold=0.7)
                }
                st.success("âœ… DeepEval æŒ‡æ¨™å·²åˆå§‹åŒ–")
            except Exception as e:
                st.warning(f"âš ï¸ DeepEval æŒ‡æ¨™åˆå§‹åŒ–å¤±æ•—: {e}")
                self.metrics = {}
    
    def setup_chatbot(self, dataset_id: str, dataset_name: str) -> bool:
        """è¨­ç½®èŠå¤©æ©Ÿå™¨äºº"""
        try:
            self.chatbot = RAGFlowChatbot()
            return self.chatbot.setup_chat(dataset_id, dataset_name)
        except Exception as e:
            st.error(f"èŠå¤©æ©Ÿå™¨äººè¨­ç½®å¤±æ•—: {e}")
            return False
    
    def generate_simple_questions(self, dataset_name: str, num_questions: int) -> List[Dict[str, str]]:
        """ç”Ÿæˆç°¡å–®çš„æ¸¬è©¦å•é¡Œ"""
        
        # æ ¹æ“šæ•¸æ“šé›†åç¨±é¸æ“‡å•é¡Œæ¨¡æ¿
        if any(keyword in dataset_name.lower() for keyword in ['æ³•å¾‹', 'legal', 'æ†²æ³•', 'æ°‘æ³•', 'è¡Œæ”¿æ³•']):
            base_questions = [
                "ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ",
                "æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "åˆ‘æ³•çš„ç½ªåˆ‘æ³•å®šåŸå‰‡å¦‚ä½•ç†è§£ï¼Ÿ",
                "è¡Œæ”¿æ³•ä¸­çš„æ¯”ä¾‹åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "æ°‘äº‹è¨´è¨Ÿä¸­çš„èˆ‰è­‰è²¬ä»»å¦‚ä½•åˆ†é…ï¼Ÿ",
                "æ†²æ³•ä¿éšœå“ªäº›åŸºæœ¬æ¬Šåˆ©ï¼Ÿ",
                "ä»€éº¼æ˜¯æ³•æ²»åœ‹å®¶çš„ç‰¹å¾µï¼Ÿ"
            ]
        elif any(keyword in dataset_name.lower() for keyword in ['æŠ€è¡“', 'tech', 'api']):
            base_questions = [
                "ä»€éº¼æ˜¯ APIï¼Ÿ",
                "å¦‚ä½•é€²è¡Œç³»çµ±é…ç½®ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æŠ€è¡“æ¦‚å¿µï¼Ÿ",
                "å¦‚ä½•è§£æ±ºå¸¸è¦‹å•é¡Œï¼Ÿ",
                "ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹ï¼Ÿ",
                "å¦‚ä½•é€²è¡Œæ€§èƒ½å„ªåŒ–ï¼Ÿ",
                "ç³»çµ±å®‰å…¨æœ‰å“ªäº›è€ƒæ…®ï¼Ÿ"
            ]
        else:
            base_questions = [
                f"{dataset_name}åŒ…å«ä»€éº¼ä¸»è¦å…§å®¹ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æ¦‚å¿µéœ€è¦äº†è§£ï¼Ÿ",
                "é€™å€‹é ˜åŸŸçš„æœ€æ–°ç™¼å±•è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ",
                "æœ‰ä»€éº¼å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ï¼Ÿ",
                "å¦‚ä½•é–‹å§‹å­¸ç¿’é€™å€‹é ˜åŸŸï¼Ÿ",
                "å¸¸è¦‹çš„å•é¡Œæœ‰å“ªäº›ï¼Ÿ",
                "æœ€ä½³å¯¦è¸æ˜¯ä»€éº¼ï¼Ÿ"
            ]
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        test_data = []
        for i in range(min(num_questions, len(base_questions))):
            question_text = base_questions[i]
            
            # ç¢ºä¿ question_text æ˜¯å­—ç¬¦ä¸²
            if not isinstance(question_text, str):
                question_text = str(question_text)
            
            test_data.append({
                'id': f"simple_{i+1}",
                'question': question_text,
                'expected_answer': f"é—œæ–¼{dataset_name}çš„ç›¸é—œå›ç­”",
                'source': 'simple_generation'
            })
        
        return test_data
    
    def get_rag_answer(self, question: str) -> Dict[str, Any]:
        """ç²å– RAG ç³»çµ±çš„å›ç­”"""
        if not self.chatbot:
            return {
                'success': False,
                'message': 'èŠå¤©æ©Ÿå™¨äººæœªåˆå§‹åŒ–'
            }
        
        try:
            result = self.chatbot.ask(question)
            return result
        except Exception as e:
            return {
                'success': False,
                'message': str(e)
            }
    
    def evaluate_with_deepeval(self, question: str, actual_output: str, 
                              expected_output: str, retrieval_context: List[str]) -> Dict[str, Any]:
        """ä½¿ç”¨ DeepEval é€²è¡Œè©•ä¼°"""
        if not self.metrics:
            return {
                'error': 'DeepEval æŒ‡æ¨™æœªåˆå§‹åŒ–',
                'metrics_scores': {}
            }
        
        try:
            # å‰µå»ºæ¸¬è©¦æ¡ˆä¾‹
            llm_test_case = LLMTestCase(
                input=question,
                actual_output=actual_output,
                expected_output=expected_output,
                retrieval_context=retrieval_context
            )
            
            # åŸ·è¡Œè©•ä¼°
            metrics_scores = {}
            for metric_name, metric in self.metrics.items():
                try:
                    metric.measure(llm_test_case)
                    metrics_scores[metric_name] = {
                        'score': metric.score,
                        'passed': metric.is_successful(),
                        'threshold': metric.threshold
                    }
                except Exception as e:
                    metrics_scores[metric_name] = {
                        'score': 0.0,
                        'passed': False,
                        'threshold': metric.threshold,
                        'error': str(e)
                    }
            
            return {
                'metrics_scores': metrics_scores
            }
            
        except Exception as e:
            return {
                'error': str(e),
                'metrics_scores': {}
            }

def main():
    """ä¸»å‡½æ•¸"""
    st.set_page_config(
        page_title="ä¿®å¾©ç‰ˆ RAG è©•ä¼°",
        page_icon="ğŸ”§",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("ğŸ”§ ä¿®å¾©ç‰ˆ RAG è©•ä¼°ç³»çµ±")
    st.markdown("å¾¹åº•è§£æ±ºå­—ç¬¦ä¸²åˆ‡ç‰‡å•é¡Œçš„ç‰ˆæœ¬")
    
    # æª¢æŸ¥ OpenAI API Key
    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        st.warning("âš ï¸ æœªè¨­ç½® OPENAI_API_KEYï¼Œå°‡ä½¿ç”¨åŸºç¤è©•ä¼°æ¨¡å¼")
        st.info("ğŸ’¡ è¨­ç½®æ–¹æ³•: export OPENAI_API_KEY='your-api-key'")
    else:
        st.success("âœ… OpenAI API Key å·²è¨­ç½®")
    
    # å´é‚Šæ¬„æ§åˆ¶
    with st.sidebar:
        st.header("âš™ï¸ æ§åˆ¶é¢æ¿")
        
        # åˆå§‹åŒ–è©•ä¼°å™¨
        if 'evaluator' not in st.session_state:
            st.session_state.evaluator = SimpleRAGEvaluator()
        
        evaluator = st.session_state.evaluator
        
        # ç²å–æ•¸æ“šé›†
        st.subheader("ğŸ“š æ•¸æ“šé›†é¸æ“‡")
        
        datasets_result = evaluator.client.list_datasets()
        if not datasets_result['success']:
            st.error(f"âŒ ç²å–æ•¸æ“šé›†å¤±æ•—: {datasets_result['message']}")
            return
        
        datasets = datasets_result['data']
        if not datasets:
            st.error("âŒ æ²’æœ‰å¯ç”¨çš„æ•¸æ“šé›†")
            return
        
        # æ•¸æ“šé›†é¸æ“‡
        dataset_names = [f"{ds.get('name', 'N/A')} ({ds.get('document_count', 'N/A')} æ–‡æª”)" for ds in datasets]
        selected_index = st.selectbox(
            "é¸æ“‡æ•¸æ“šé›†:", 
            range(len(dataset_names)), 
            format_func=lambda x: dataset_names[x]
        )
        
        selected_dataset = datasets[selected_index]
        st.session_state.selected_dataset = selected_dataset
        
        # è¨­ç½®èŠå¤©æ©Ÿå™¨äºº
        if st.session_state.get('current_dataset_id') != selected_dataset['id']:
            if evaluator.setup_chatbot(selected_dataset['id'], selected_dataset['name']):
                st.session_state.current_dataset_id = selected_dataset['id']
                st.success("âœ… RAG ç³»çµ±å·²é€£æ¥")
            else:
                st.error("âŒ èŠå¤©æ©Ÿå™¨äººè¨­ç½®å¤±æ•—")
                return
        
        st.divider()
        
        # è©•ä¼°é…ç½®
        st.subheader("ğŸ¯ è©•ä¼°é…ç½®")
        num_questions = st.slider("æ¸¬è©¦å•é¡Œæ•¸é‡:", 1, 10, 3)
        
        st.divider()
        
        # æ“ä½œæŒ‰éˆ•
        st.subheader("ğŸš€ æ“ä½œ")
        
        # ç”Ÿæˆæ¸¬è©¦å•é¡Œ
        if st.button("ğŸ“ ç”Ÿæˆæ¸¬è©¦å•é¡Œ", use_container_width=True):
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ¸¬è©¦å•é¡Œ..."):
                try:
                    test_data = evaluator.generate_simple_questions(
                        selected_dataset['name'], num_questions
                    )
                    st.session_state.test_data = test_data
                    st.success(f"âœ… ç”Ÿæˆäº† {len(test_data)} å€‹æ¸¬è©¦å•é¡Œ")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ ç”Ÿæˆæ¸¬è©¦å•é¡Œå¤±æ•—: {e}")
        
        # åŸ·è¡Œè©•ä¼°
        if 'test_data' in st.session_state and st.session_state.test_data:
            st.success(f"ğŸ“ å·²æº–å‚™ {len(st.session_state.test_data)} å€‹æ¸¬è©¦å•é¡Œ")
            
            if st.button("ğŸ§ª é–‹å§‹è©•ä¼°", type="primary", use_container_width=True):
                run_evaluation(evaluator)
        
        # æ¸…é™¤æ•¸æ“š
        if 'test_data' in st.session_state or 'evaluation_results' in st.session_state:
            st.divider()
            if st.button("ğŸ—‘ï¸ æ¸…é™¤æ•¸æ“š", use_container_width=True):
                clear_session_data()
                st.success("âœ… æ•¸æ“šå·²æ¸…é™¤")
                st.rerun()
    
    # ä¸»é é¢å…§å®¹
    display_main_content()

def run_evaluation(evaluator):
    """åŸ·è¡Œè©•ä¼°"""
    test_data = st.session_state.test_data
    
    results = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, test_case in enumerate(test_data):
        # ç¢ºä¿ question æ˜¯å­—ç¬¦ä¸²
        question = str(test_case['question'])
        
        status_text.text(f"ğŸ”„ è©•ä¼°æ¡ˆä¾‹ {i+1}/{len(test_data)}: {question[:50]}...")
        progress_bar.progress((i + 1) / len(test_data))
        
        # ç²å– RAG å›ç­”
        rag_result = evaluator.get_rag_answer(question)
        
        if rag_result['success']:
            actual_output = rag_result['answer']
            expected_output = test_case['expected_answer']
            retrieval_context = [
                source.get('content', '') for source in rag_result.get('sources', [])
                if isinstance(source, dict) and source.get('content')
            ]
            
            # DeepEval è©•ä¼°
            if evaluator.metrics:
                eval_result = evaluator.evaluate_with_deepeval(
                    question, actual_output, expected_output, retrieval_context
                )
                metrics_scores = eval_result.get('metrics_scores', {})
            else:
                # ç„¡è©•ä¼°æŒ‡æ¨™å¯ç”¨
                st.error("âŒ ç„¡æ³•é€²è¡Œè©•ä¼°ï¼šDeepEval æŒ‡æ¨™æœªåˆå§‹åŒ–")
                return
            
            # è¨ˆç®—æ•´é«”çµæœ
            passed_count = sum(1 for m in metrics_scores.values() if m.get('passed', False))
            total_count = len(metrics_scores)
            overall_passed = passed_count >= total_count * 0.5
            avg_score = sum(m.get('score', 0) for m in metrics_scores.values()) / total_count if total_count > 0 else 0
            
            results.append({
                'test_case_id': test_case['id'],
                'question': question,
                'actual_output': actual_output,
                'expected_output': expected_output,
                'retrieval_context': retrieval_context,
                'metrics_scores': metrics_scores,
                'overall_score': avg_score,
                'passed': overall_passed
            })
        else:
            st.error(f"âŒ æ¡ˆä¾‹ {i+1} RAG æŸ¥è©¢å¤±æ•—: {rag_result.get('message', 'æœªçŸ¥éŒ¯èª¤')}")
    
    # æ¸…é™¤é€²åº¦é¡¯ç¤º
    progress_bar.empty()
    status_text.empty()
    
    # ä¿å­˜çµæœ
    st.session_state.evaluation_results = results
    st.success(f"ğŸ‰ è©•ä¼°å®Œæˆï¼å…±è©•ä¼°äº† {len(results)} å€‹æ¡ˆä¾‹")
    st.rerun()

def display_main_content():
    """é¡¯ç¤ºä¸»é é¢å…§å®¹"""
    if 'evaluation_results' in st.session_state:
        display_evaluation_results()
    elif 'test_data' in st.session_state:
        display_test_questions()
    else:
        display_welcome_page()

def display_welcome_page():
    """é¡¯ç¤ºæ­¡è¿é é¢"""
    st.markdown("""
    ## ğŸ”§ ä¿®å¾©ç‰ˆ RAG è©•ä¼°ç³»çµ±
    
    é€™æ˜¯ä¸€å€‹å¾¹åº•è§£æ±ºå­—ç¬¦ä¸²åˆ‡ç‰‡å•é¡Œçš„ RAG è©•ä¼°ç³»çµ±ï¼š
    
    ### âœ¨ ä¸»è¦ç‰¹è‰²
    - ğŸ”§ **å•é¡Œä¿®å¾©**: å¾¹åº•è§£æ±ºå­—ç¬¦ä¸²åˆ‡ç‰‡éŒ¯èª¤
    - ğŸ›¡ï¸ **éŒ¯èª¤è™•ç†**: å®Œå–„çš„ç•°å¸¸è™•ç†æ©Ÿåˆ¶
    - ğŸ¯ **ç°¡åŒ–é‚è¼¯**: é¿å…è¤‡é›œçš„å•é¡Œç”Ÿæˆé‚è¼¯
    - ğŸ“Š **éˆæ´»è©•ä¼°**: æ”¯æŒæœ‰/ç„¡ OpenAI API Key çš„è©•ä¼°æ¨¡å¼
    
    ### ğŸ“‹ ä½¿ç”¨æ­¥é©Ÿ
    1. åœ¨å·¦å´é‚Šæ¬„é¸æ“‡æ•¸æ“šé›†
    2. è¨­å®šæ¸¬è©¦å•é¡Œæ•¸é‡
    3. ç”Ÿæˆæ¸¬è©¦å•é¡Œ
    4. åŸ·è¡Œè©•ä¼°
    5. æŸ¥çœ‹çµæœ
    
    **ğŸ‘ˆ è«‹åœ¨å·¦å´é‚Šæ¬„é–‹å§‹æ“ä½œï¼**
    """)

def display_test_questions():
    """é¡¯ç¤ºæ¸¬è©¦å•é¡Œé è¦½"""
    st.subheader("â“ æ¸¬è©¦å•é¡Œé è¦½")
    
    test_data = st.session_state.test_data
    
    # çµ±è¨ˆä¿¡æ¯
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("å•é¡Œç¸½æ•¸", len(test_data))
    with col2:
        avg_len = sum(len(str(q['question'])) for q in test_data) / len(test_data)
        st.metric("å¹³å‡å•é¡Œé•·åº¦", f"{avg_len:.0f} å­—ç¬¦")
    with col3:
        st.metric("æ•¸æ“šä¾†æº", test_data[0].get('source', 'unknown'))
    
    # å•é¡Œåˆ—è¡¨
    for i, data in enumerate(test_data):
        question = str(data['question'])  # ç¢ºä¿æ˜¯å­—ç¬¦ä¸²
        with st.expander(f"å•é¡Œ {i+1}: {question[:60]}...", expanded=False):
            st.write("**å•é¡Œ:**")
            st.write(question)
            st.write("**æœŸæœ›ç­”æ¡ˆ:**")
            st.write(data['expected_answer'])
    
    st.info("ğŸ’¡ å•é¡Œå·²æº–å‚™å°±ç·’ï¼Œè«‹åœ¨å·¦å´é‚Šæ¬„é»æ“Šã€Œé–‹å§‹è©•ä¼°ã€æŒ‰éˆ•ï¼")

def display_evaluation_results():
    """é¡¯ç¤ºè©•ä¼°çµæœ"""
    results = st.session_state.evaluation_results
    
    st.subheader("ğŸ“Š è©•ä¼°çµæœ")
    
    # æ•´é«”çµ±è¨ˆ
    total_cases = len(results)
    passed_cases = sum(1 for r in results if r['passed'])
    pass_rate = passed_cases / total_cases * 100
    avg_score = sum(r['overall_score'] for r in results) / total_cases
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç¸½æ¡ˆä¾‹æ•¸", total_cases)
    with col2:
        st.metric("é€šéæ¡ˆä¾‹", passed_cases)
    with col3:
        st.metric("é€šéç‡", f"{pass_rate:.1f}%")
    with col4:
        st.metric("å¹³å‡åˆ†æ•¸", f"{avg_score:.3f}")
    
    # è©³ç´°çµæœ
    st.subheader("ğŸ“‹ è©³ç´°çµæœ")
    
    for i, result in enumerate(results):
        status = "âœ… é€šé" if result['passed'] else "âŒ å¤±æ•—"
        question = str(result['question'])  # ç¢ºä¿æ˜¯å­—ç¬¦ä¸²
        
        with st.expander(f"{status} | æ¡ˆä¾‹ {i+1} | åˆ†æ•¸: {result['overall_score']:.3f}", expanded=False):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write("**å•é¡Œ:**")
                st.write(question)
                
                st.write("**RAG ç³»çµ±å›ç­”:**")
                st.write(result['actual_output'])
                
                st.write("**æœŸæœ›å›ç­”:**")
                st.write(result['expected_output'])
            
            with col2:
                st.write("**è©•ä¼°æŒ‡æ¨™:**")
                
                for metric_name, metric_data in result['metrics_scores'].items():
                    if 'error' in metric_data:
                        st.write(f"âŒ **{metric_name}**: éŒ¯èª¤")
                    else:
                        score = metric_data.get('score', 0)
                        passed = metric_data.get('passed', False)
                        threshold = metric_data.get('threshold', 0)
                        
                        status_icon = "âœ…" if passed else "âŒ"
                        st.write(f"{status_icon} **{metric_name}**: {score:.3f}")
                        st.write(f"   é–¾å€¼: {threshold}")

def clear_session_data():
    """æ¸…é™¤æœƒè©±æ•¸æ“š"""
    keys_to_clear = ['test_data', 'evaluation_results']
    for key in keys_to_clear:
        if key in st.session_state:
            del st.session_state[key]

if __name__ == "__main__":
    main()