#!/usr/bin/env python3
"""
DeepEval æ•´åˆæ¨¡çµ„ - RAGFlow èŠå¤©æ©Ÿå™¨äººè©•ä¼°ç³»çµ±
è‡ªå‹•ç”Ÿæˆå•ç­”æ•¸æ“šä¸¦è©•ä¼° RAG ç³»çµ±æ€§èƒ½
"""

import os
import json
import asyncio
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import pandas as pd

# DeepEval imports
from deepeval import evaluate
from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    ContextualPrecisionMetric,
    ContextualRecallMetric,
    HallucinationMetric,
    BiasMetric
)
from deepeval.test_case import LLMTestCase
from deepeval.synthesizer import Synthesizer

# æœ¬åœ°æ¨¡çµ„
from ragflow_chatbot import RAGFlowChatbot, RAGFlowOfficialClient

@dataclass
class EvaluationResult:
    """è©•ä¼°çµæœæ•¸æ“šé¡"""
    test_case_id: str
    question: str
    actual_output: str
    expected_output: str
    retrieval_context: List[str]
    metrics_scores: Dict[str, float]
    overall_score: float
    passed: bool

class RAGFlowEvaluator:
    """RAGFlow ç³»çµ±è©•ä¼°å™¨"""
    
    def __init__(self, openai_api_key: str = None):
        """
        åˆå§‹åŒ–è©•ä¼°å™¨
        
        Args:
            openai_api_key: OpenAI API å¯†é‘° (ç”¨æ–¼ DeepEval)
        """
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            print("âš ï¸  è­¦å‘Š: æœªè¨­ç½® OPENAI_API_KEYï¼ŒæŸäº›è©•ä¼°åŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨")
        
        # è¨­ç½® OpenAI API å¯†é‘°
        if self.openai_api_key:
            os.environ["OPENAI_API_KEY"] = self.openai_api_key
        
        self.client = RAGFlowOfficialClient()
        self.chatbot = None
        self.synthesizer = None
        
        # åªæœ‰åœ¨æœ‰ API Key æ™‚æ‰åˆå§‹åŒ–è©•ä¼°æŒ‡æ¨™
        self.metrics = {}
        if self.openai_api_key:
            try:
                self.metrics = {
                    'answer_relevancy': AnswerRelevancyMetric(threshold=0.7),
                    'faithfulness': FaithfulnessMetric(threshold=0.7),
                    'contextual_precision': ContextualPrecisionMetric(threshold=0.7),
                    'contextual_recall': ContextualRecallMetric(threshold=0.7),
                    'hallucination': HallucinationMetric(threshold=0.3),
                    'bias': BiasMetric(threshold=0.5)
                }
                print("âœ… DeepEval æŒ‡æ¨™åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  DeepEval æŒ‡æ¨™åˆå§‹åŒ–å¤±æ•—: {e}")
                self.metrics = {}
        else:
            print("â„¹ï¸  è·³é DeepEval æŒ‡æ¨™åˆå§‹åŒ–ï¼ˆéœ€è¦ OpenAI API Keyï¼‰")
    
    def setup_chatbot(self, dataset_id: str, dataset_name: str = "Unknown") -> bool:
        """è¨­ç½®èŠå¤©æ©Ÿå™¨äºº"""
        self.chatbot = RAGFlowChatbot()
        return self.chatbot.setup_chat(dataset_id, dataset_name)
    
    def generate_test_data_from_documents(self, dataset_id: str, num_questions: int = 10) -> List[Dict[str, Any]]:
        """
        å¾æ–‡æª”ç”Ÿæˆæ¸¬è©¦å•ç­”æ•¸æ“š
        
        Args:
            dataset_id: æ•¸æ“šé›† ID
            num_questions: ç”Ÿæˆå•é¡Œæ•¸é‡
            
        Returns:
            ç”Ÿæˆçš„æ¸¬è©¦æ•¸æ“šåˆ—è¡¨
        """
        print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆ {num_questions} å€‹æ¸¬è©¦å•é¡Œ...")
        
        # å¦‚æœæœ‰ OpenAI APIï¼Œä½¿ç”¨ DeepEval Synthesizer
        if self.openai_api_key:
            try:
                return self._generate_with_synthesizer(dataset_id, num_questions)
            except Exception as e:
                print(f"âš ï¸  Synthesizer ç”Ÿæˆå¤±æ•—: {e}")
                print("ğŸ”„ åˆ‡æ›åˆ°æ‰‹å‹•ç”Ÿæˆæ¨¡å¼...")
        
        # æ‰‹å‹•ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        return self._generate_manual_test_data(dataset_id, num_questions)
    
    def _generate_with_synthesizer(self, dataset_id: str, num_questions: int) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ DeepEval Synthesizer ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
        if not self.synthesizer:
            self.synthesizer = Synthesizer()
        
        # ç²å–æ•¸æ“šé›†ä¿¡æ¯
        datasets_result = self.client.list_datasets()
        if not datasets_result['success']:
            raise Exception("ç„¡æ³•ç²å–æ•¸æ“šé›†ä¿¡æ¯")
        
        dataset_info = None
        for ds in datasets_result['data']:
            if ds['id'] == dataset_id:
                dataset_info = ds
                break
        
        if not dataset_info:
            raise Exception(f"æ‰¾ä¸åˆ°æ•¸æ“šé›† {dataset_id}")
        
        # å˜—è©¦ç²å–å¯¦éš›çš„æ–‡æª”å…§å®¹ä½œç‚ºä¸Šä¸‹æ–‡
        try:
            # ä½¿ç”¨èŠå¤©æ©Ÿå™¨äººç²å–ä¸€äº›ç¤ºä¾‹å…§å®¹
            sample_questions = [
                "é€™å€‹çŸ¥è­˜åº«åŒ…å«ä»€éº¼å…§å®¹ï¼Ÿ",
                "ä¸»è¦æ¶µè“‹å“ªäº›ä¸»é¡Œï¼Ÿ",
                "æœ‰ä»€éº¼é‡è¦ä¿¡æ¯ï¼Ÿ"
            ]
            
            contexts = []
            for question in sample_questions[:2]:  # åªå–å‰2å€‹å•é¡Œé¿å…éå¤šè«‹æ±‚
                result = self.chatbot.ask(question)
                if result['success'] and result['sources']:
                    for source in result['sources'][:2]:  # æ¯å€‹å•é¡Œå–å‰2å€‹ä¾†æº
                        if isinstance(source, dict) and source.get('content'):
                            content = source['content'].strip()
                            if content and len(content) > 50:  # ç¢ºä¿å…§å®¹æœ‰æ„ç¾©
                                contexts.append(content[:500])  # é™åˆ¶é•·åº¦
            
            # å¦‚æœæ²’æœ‰ç²å–åˆ°è¶³å¤ çš„ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨åŸºæœ¬æè¿°
            if not contexts:
                contexts = [
                    f"é€™æ˜¯ä¸€å€‹é—œæ–¼{dataset_info['name']}çš„çŸ¥è­˜åº«ï¼ŒåŒ…å«{dataset_info.get('document_count', 'å¤šå€‹')}å€‹æ–‡æª”ã€‚",
                    f"çŸ¥è­˜åº«æ¶µè“‹äº†{dataset_info['name']}ç›¸é—œçš„å„ç¨®ä¿¡æ¯å’Œå…§å®¹ã€‚"
                ]
            
            print(f"ğŸ“š ä½¿ç”¨ {len(contexts)} å€‹ä¸Šä¸‹æ–‡ç”Ÿæˆæ¸¬è©¦å•é¡Œ")
            
        except Exception as e:
            print(f"âš ï¸ ç²å–ä¸Šä¸‹æ–‡å¤±æ•—: {e}")
            # ä½¿ç”¨åŸºæœ¬ä¸Šä¸‹æ–‡
            contexts = [
                f"é€™æ˜¯ä¸€å€‹é—œæ–¼{dataset_info['name']}çš„çŸ¥è­˜åº«ï¼ŒåŒ…å«{dataset_info.get('document_count', 'å¤šå€‹')}å€‹æ–‡æª”ã€‚",
                f"çŸ¥è­˜åº«æ¶µè“‹äº†{dataset_info['name']}ç›¸é—œçš„å„ç¨®ä¿¡æ¯å’Œå…§å®¹ã€‚"
            ]
        
        # ç”Ÿæˆåˆæˆæ•¸æ“š
        synthetic_data = self.synthesizer.generate_goldens_from_contexts(
            contexts=contexts,
            max_goldens_per_context=max(1, num_questions // len(contexts))
        )
        
        test_data = []
        for i, golden in enumerate(synthetic_data):
            test_data.append({
                'id': f"synthetic_{i+1}",
                'question': golden.input,
                'expected_answer': golden.expected_output,
                'context': golden.context if isinstance(golden.context, str) else str(golden.context),
                'source': 'synthesizer'
            })
        
        return test_data[:num_questions]  # ç¢ºä¿ä¸è¶…éè«‹æ±‚çš„æ•¸é‡
    
    def _generate_manual_test_data(self, dataset_id: str, num_questions: int) -> List[Dict[str, Any]]:
        """æ‰‹å‹•ç”Ÿæˆæ¸¬è©¦æ•¸æ“š"""
        print("ğŸ”„ ä½¿ç”¨æ‰‹å‹•ç”Ÿæˆæ¨¡å¼...")
        
        # ç²å–æ•¸æ“šé›†ä¿¡æ¯
        datasets_result = self.client.list_datasets()
        if not datasets_result['success']:
            raise Exception("ç„¡æ³•ç²å–æ•¸æ“šé›†ä¿¡æ¯")
        
        dataset_info = None
        for ds in datasets_result['data']:
            if ds['id'] == dataset_id:
                dataset_info = ds
                break
        
        if not dataset_info:
            raise Exception(f"æ‰¾ä¸åˆ°æ•¸æ“šé›† {dataset_id}")
        
        dataset_name = dataset_info['name']
        print(f"ğŸ“š ç‚ºæ•¸æ“šé›† '{dataset_name}' ç”Ÿæˆ {num_questions} å€‹æ¸¬è©¦å•é¡Œ")
        
        # ä½¿ç”¨ç°¡å–®çš„å•é¡Œåˆ—è¡¨ï¼Œé¿å…è¤‡é›œçš„ç”Ÿæˆé‚è¼¯
        if "æ³•å¾‹" in dataset_name or "legal" in dataset_name.lower() or "æ†²æ³•" in dataset_name or "æ°‘æ³•" in dataset_name:
            simple_questions = [
                "ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ",
                "æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "åˆ‘æ³•çš„ç½ªåˆ‘æ³•å®šåŸå‰‡å¦‚ä½•ç†è§£ï¼Ÿ",
                "è¡Œæ”¿æ³•ä¸­çš„æ¯”ä¾‹åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ",
                "æ°‘äº‹è¨´è¨Ÿä¸­çš„èˆ‰è­‰è²¬ä»»å¦‚ä½•åˆ†é…ï¼Ÿ"
            ]
        elif "æŠ€è¡“" in dataset_name or "tech" in dataset_name.lower() or "api" in dataset_name.lower():
            simple_questions = [
                "ä»€éº¼æ˜¯ APIï¼Ÿ",
                "å¦‚ä½•é€²è¡Œç³»çµ±é…ç½®ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æŠ€è¡“æ¦‚å¿µï¼Ÿ",
                "å¦‚ä½•è§£æ±ºå¸¸è¦‹å•é¡Œï¼Ÿ",
                "ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹ï¼Ÿ"
            ]
        else:
            simple_questions = [
                f"{dataset_name} åŒ…å«ä»€éº¼ä¸»è¦å…§å®¹ï¼Ÿ",
                "æœ‰å“ªäº›é‡è¦çš„æ¦‚å¿µéœ€è¦äº†è§£ï¼Ÿ",
                "é€™å€‹é ˜åŸŸçš„æœ€æ–°ç™¼å±•è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ",
                "æœ‰ä»€éº¼å¯¦éš›æ‡‰ç”¨æ¡ˆä¾‹ï¼Ÿ",
                "å¦‚ä½•é–‹å§‹å­¸ç¿’é€™å€‹é ˜åŸŸï¼Ÿ"
            ]
        
        # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
        test_data = []
        for i in range(min(num_questions, len(simple_questions))):
            question_text = simple_questions[i]
            print(f"ğŸ” ç”Ÿæˆå•é¡Œ {i+1}/{num_questions}: {question_text[:50]}...")
            
            # å‘ RAG ç³»çµ±è©¢å•ä»¥ç²å–æœŸæœ›ç­”æ¡ˆ
            try:
                result = self.chatbot.ask(question_text)
                if result['success']:
                    expected_answer = result['answer']
                    context = result['sources'][0].get('content', '') if result['sources'] else f"ä¾†è‡ª {dataset_name} çŸ¥è­˜åº«çš„å…§å®¹"
                else:
                    expected_answer = f"é—œæ–¼ {dataset_name} çš„ç›¸é—œä¿¡æ¯"
                    context = f"ä¾†è‡ª {dataset_name} çŸ¥è­˜åº«çš„å…§å®¹"
            except Exception as e:
                print(f"âš ï¸ ç²å–ç­”æ¡ˆå¤±æ•—: {e}")
                expected_answer = f"é—œæ–¼ {dataset_name} çš„ç›¸é—œä¿¡æ¯"
                context = f"ä¾†è‡ª {dataset_name} çŸ¥è­˜åº«çš„å…§å®¹"
            
            test_data.append({
                'id': f"manual_{i+1}",
                'question': question_text,
                'expected_answer': expected_answer,
                'context': context,
                'source': 'manual'
            })
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_data)} å€‹æ¸¬è©¦å•é¡Œ")
        return test_data
        
        test_data = []
        for i, q in enumerate(questions[:num_questions]):
            test_data.append({
                'id': f"manual_{i+1}",
                'question': q['question'],
                'expected_answer': q['expected_answer'],
                'context': q['context'],
                'source': 'manual'
            })
        
        return test_data
    
    def _generate_legal_questions(self, dataset_name: str, num_questions: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ³•å¾‹ç›¸é—œå•é¡Œ"""
        base_questions = [
            {
                'question': 'ä»€éº¼æ˜¯æ†²æ³•çš„åŸºæœ¬åŸå‰‡ï¼Ÿ',
                'expected_answer': 'æ†²æ³•çš„åŸºæœ¬åŸå‰‡åŒ…æ‹¬äººæ°‘ä¸»æ¬Šã€æ¬ŠåŠ›åˆ†ç«‹ã€åŸºæœ¬äººæ¬Šä¿éšœç­‰æ ¸å¿ƒæ¦‚å¿µã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„æ†²æ³•ç›¸é—œå…§å®¹'
            },
            {
                'question': 'æ°‘æ³•ä¸­çš„å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ',
                'expected_answer': 'å¥‘ç´„è‡ªç”±åŸå‰‡æ˜¯æŒ‡ç•¶äº‹äººæœ‰è‡ªç”±æ±ºå®šæ˜¯å¦ç· çµå¥‘ç´„ã€èˆ‡ä½•äººç· çµå¥‘ç´„ã€å¥‘ç´„å…§å®¹ç­‰çš„æ¬Šåˆ©ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„æ°‘æ³•ç›¸é—œå…§å®¹'
            },
            {
                'question': 'åˆ‘æ³•çš„ç½ªåˆ‘æ³•å®šåŸå‰‡å¦‚ä½•ç†è§£ï¼Ÿ',
                'expected_answer': 'ç½ªåˆ‘æ³•å®šåŸå‰‡æ˜¯æŒ‡çŠ¯ç½ªå’Œåˆ‘ç½°å¿…é ˆç”±æ³•å¾‹æ˜æ–‡è¦å®šï¼Œä¸å¾—ä»»æ„æ“´å¼µè§£é‡‹ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„åˆ‘æ³•ç›¸é—œå…§å®¹'
            },
            {
                'question': 'è¡Œæ”¿æ³•ä¸­çš„æ¯”ä¾‹åŸå‰‡æ˜¯ä»€éº¼ï¼Ÿ',
                'expected_answer': 'æ¯”ä¾‹åŸå‰‡è¦æ±‚è¡Œæ”¿è¡Œç‚ºå¿…é ˆé©ç•¶ã€å¿…è¦ä¸”ä¸éåº¦ï¼Œä»¥é”æˆè¡Œæ”¿ç›®çš„ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„è¡Œæ”¿æ³•ç›¸é—œå…§å®¹'
            },
            {
                'question': 'æ°‘äº‹è¨´è¨Ÿä¸­çš„èˆ‰è­‰è²¬ä»»å¦‚ä½•åˆ†é…ï¼Ÿ',
                'expected_answer': 'èˆ‰è­‰è²¬ä»»åŸå‰‡ä¸Šç”±ä¸»å¼µäº‹å¯¦å­˜åœ¨çš„ç•¶äº‹äººè² æ“”ï¼Œä½†æ³•å¾‹å¦æœ‰è¦å®šè€…é™¤å¤–ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„æ°‘äº‹è¨´è¨Ÿæ³•ç›¸é—œå…§å®¹'
            }
        ]
        
        # æ“´å±•å•é¡Œåˆ—è¡¨
        extended_questions = base_questions * (num_questions // len(base_questions) + 1)
        return extended_questions[:num_questions]
    
    def _generate_tech_questions(self, dataset_name: str, num_questions: int) -> List[Dict[str, Any]]:
        """ç”ŸæˆæŠ€è¡“ç›¸é—œå•é¡Œ"""
        base_questions = [
            {
                'question': 'ä»€éº¼æ˜¯ APIï¼Ÿ',
                'expected_answer': 'API (Application Programming Interface) æ˜¯æ‡‰ç”¨ç¨‹å¼ä»‹é¢ï¼Œç”¨æ–¼ä¸åŒè»Ÿé«”çµ„ä»¶ä¹‹é–“çš„é€šä¿¡ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„ API ç›¸é—œå…§å®¹'
            },
            {
                'question': 'ä»€éº¼æ˜¯ RESTful æ¶æ§‹ï¼Ÿ',
                'expected_answer': 'RESTful æ˜¯ä¸€ç¨®è»Ÿé«”æ¶æ§‹é¢¨æ ¼ï¼Œä½¿ç”¨ HTTP æ–¹æ³•é€²è¡Œè³‡æºæ“ä½œï¼Œå…·æœ‰ç„¡ç‹€æ…‹ã€çµ±ä¸€ä»‹é¢ç­‰ç‰¹é»ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„ REST ç›¸é—œå…§å®¹'
            },
            {
                'question': 'ä»€éº¼æ˜¯å¾®æœå‹™æ¶æ§‹ï¼Ÿ',
                'expected_answer': 'å¾®æœå‹™æ¶æ§‹æ˜¯å°‡å¤§å‹æ‡‰ç”¨ç¨‹å¼åˆ†è§£ç‚ºå¤šå€‹å°å‹ã€ç¨ç«‹çš„æœå‹™ï¼Œæ¯å€‹æœå‹™è² è²¬ç‰¹å®šçš„æ¥­å‹™åŠŸèƒ½ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„å¾®æœå‹™ç›¸é—œå…§å®¹'
            }
        ]
        
        extended_questions = base_questions * (num_questions // len(base_questions) + 1)
        return extended_questions[:num_questions]
    
    def _generate_general_questions(self, dataset_name: str, num_questions: int) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé€šç”¨å•é¡Œ"""
        base_questions = [
            {
                'question': f'{dataset_name} åŒ…å«ä»€éº¼ä¸»è¦å…§å®¹ï¼Ÿ',
                'expected_answer': f'{dataset_name} åŒ…å«ç›¸é—œé ˜åŸŸçš„æ ¸å¿ƒçŸ¥è­˜å’Œé‡è¦æ¦‚å¿µã€‚',
                'context': f'åŸºæ–¼ {dataset_name} çš„æ•´é«”å…§å®¹æ¦‚è¿°'
            },
            {
                'question': 'æœ‰å“ªäº›é‡è¦çš„æ¦‚å¿µéœ€è¦äº†è§£ï¼Ÿ',
                'expected_answer': 'é‡è¦æ¦‚å¿µåŒ…æ‹¬åŸºç¤ç†è«–ã€å¯¦å‹™æ‡‰ç”¨å’Œç›¸é—œæ¡ˆä¾‹ç­‰ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„æ ¸å¿ƒæ¦‚å¿µ'
            },
            {
                'question': 'é€™å€‹é ˜åŸŸçš„æœ€æ–°ç™¼å±•è¶¨å‹¢æ˜¯ä»€éº¼ï¼Ÿ',
                'expected_answer': 'æœ€æ–°ç™¼å±•è¶¨å‹¢åŒ…æ‹¬æŠ€è¡“å‰µæ–°ã€æ”¿ç­–è®ŠåŒ–å’Œå¯¦å‹™æ¼”é€²ç­‰æ–¹é¢ã€‚',
                'context': f'åŸºæ–¼ {dataset_name} ä¸­çš„è¶¨å‹¢åˆ†æ'
            }
        ]
        
        extended_questions = base_questions * (num_questions // len(base_questions) + 1)
        return extended_questions[:num_questions]
    
    def evaluate_test_cases(self, test_data: List[Dict[str, Any]]) -> List[EvaluationResult]:
        """è©•ä¼°æ¸¬è©¦æ¡ˆä¾‹"""
        if not self.chatbot:
            raise Exception("èŠå¤©æ©Ÿå™¨äººæœªè¨­ç½®ï¼Œè«‹å…ˆèª¿ç”¨ setup_chatbot()")
        
        print(f"ğŸ§ª é–‹å§‹è©•ä¼° {len(test_data)} å€‹æ¸¬è©¦æ¡ˆä¾‹...")
        results = []
        
        for i, test_case in enumerate(test_data, 1):
            print(f"ğŸ“ è©•ä¼°æ¡ˆä¾‹ {i}/{len(test_data)}: {test_case['question'][:50]}...")
            
            # ç²å– RAG ç³»çµ±å›ç­”
            rag_result = self.chatbot.ask(test_case['question'])
            
            if not rag_result['success']:
                print(f"âŒ æ¡ˆä¾‹ {i} æŸ¥è©¢å¤±æ•—: {rag_result['message']}")
                continue
            
            # æº–å‚™è©•ä¼°æ•¸æ“š
            actual_output = rag_result['answer']
            expected_output = test_case['expected_answer']
            retrieval_context = [source.get('content', '') for source in rag_result['sources'] if isinstance(source, dict)]
            
            # å‰µå»º LLM æ¸¬è©¦æ¡ˆä¾‹
            llm_test_case = LLMTestCase(
                input=test_case['question'],
                actual_output=actual_output,
                expected_output=expected_output,
                retrieval_context=retrieval_context
            )
            
            # è©•ä¼°æŒ‡æ¨™
            metrics_scores = {}
            overall_score = 0
            passed_count = 0
            
            # æ ¹æ“š API å¯ç”¨æ€§å’Œè©•ä¼°æ¨¡å¼é¸æ“‡æŒ‡æ¨™
            if self.openai_api_key:
                # æœ‰ API æ™‚ä½¿ç”¨æ¨™æº–è©•ä¼°ï¼ˆé¿å…éå¤šæŒ‡æ¨™ï¼‰
                from deepeval_config import DeepEvalConfig
                metrics_to_evaluate = DeepEvalConfig.EVALUATION_STAGES.get('standard', ['answer_relevancy', 'faithfulness'])
            else:
                # ç„¡ API æ™‚è·³ééœ€è¦ LLM çš„æŒ‡æ¨™
                metrics_to_evaluate = []
            
            for metric_name, metric in self.metrics.items():
                if not metrics_to_evaluate or metric_name in metrics_to_evaluate:
                    try:
                        metric.measure(llm_test_case)
                        score = metric.score
                        metrics_scores[metric_name] = score
                        overall_score += score
                        if metric.is_successful():
                            passed_count += 1
                    except Exception as e:
                        print(f"âš ï¸  æŒ‡æ¨™ {metric_name} è©•ä¼°å¤±æ•—: {e}")
                        metrics_scores[metric_name] = 0.0
            
            # è¨ˆç®—æ•´é«”åˆ†æ•¸
            if metrics_scores:
                overall_score = overall_score / len(metrics_scores)
                passed = passed_count >= len(metrics_scores) * 0.6  # 60% é€šéç‡
            else:
                overall_score = 0.5  # é è¨­åˆ†æ•¸
                passed = len(actual_output) > 10  # ç°¡å–®çš„é€šéæ¨™æº–
            
            result = EvaluationResult(
                test_case_id=test_case['id'],
                question=test_case['question'],
                actual_output=actual_output,
                expected_output=expected_output,
                retrieval_context=retrieval_context,
                metrics_scores=metrics_scores,
                overall_score=overall_score,
                passed=passed
            )
            
            results.append(result)
            print(f"âœ… æ¡ˆä¾‹ {i} å®Œæˆï¼Œåˆ†æ•¸: {overall_score:.2f}")
        
        return results
    
    def save_results(self, results: List[EvaluationResult], filename: str = "evaluation_results.json"):
        """ä¿å­˜è©•ä¼°çµæœ"""
        results_data = []
        for result in results:
            results_data.append({
                'test_case_id': result.test_case_id,
                'question': result.question,
                'actual_output': result.actual_output,
                'expected_output': result.expected_output,
                'retrieval_context': result.retrieval_context,
                'metrics_scores': result.metrics_scores,
                'overall_score': result.overall_score,
                'passed': result.passed
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜åˆ° {filename}")
    
    def generate_report(self, results: List[EvaluationResult]) -> str:
        """ç”Ÿæˆè©•ä¼°å ±å‘Š"""
        if not results:
            return "æ²’æœ‰è©•ä¼°çµæœ"
        
        total_cases = len(results)
        passed_cases = sum(1 for r in results if r.passed)
        pass_rate = passed_cases / total_cases * 100
        
        avg_score = sum(r.overall_score for r in results) / total_cases
        
        # æŒ‡æ¨™çµ±è¨ˆ
        all_metrics = set()
        for result in results:
            all_metrics.update(result.metrics_scores.keys())
        
        metric_stats = {}
        for metric in all_metrics:
            scores = [r.metrics_scores.get(metric, 0) for r in results if metric in r.metrics_scores]
            if scores:
                metric_stats[metric] = {
                    'avg': sum(scores) / len(scores),
                    'min': min(scores),
                    'max': max(scores)
                }
        
        report = f"""
ğŸ“Š RAGFlow ç³»çµ±è©•ä¼°å ±å‘Š
{'='*50}

ğŸ“ˆ æ•´é«”çµ±è¨ˆ:
- ç¸½æ¸¬è©¦æ¡ˆä¾‹: {total_cases}
- é€šéæ¡ˆä¾‹: {passed_cases}
- é€šéç‡: {pass_rate:.1f}%
- å¹³å‡åˆ†æ•¸: {avg_score:.3f}

ğŸ“‹ æŒ‡æ¨™è©³æƒ…:
"""
        
        for metric, stats in metric_stats.items():
            report += f"- {metric}: å¹³å‡ {stats['avg']:.3f} (ç¯„åœ: {stats['min']:.3f} - {stats['max']:.3f})\n"
        
        report += f"""
ğŸ” è©³ç´°çµæœ:
"""
        
        for i, result in enumerate(results, 1):
            status = "âœ… é€šé" if result.passed else "âŒ å¤±æ•—"
            report += f"{i}. {status} | åˆ†æ•¸: {result.overall_score:.3f} | {result.question[:50]}...\n"
        
        return report

def main():
    """ä¸»å‡½æ•¸ - æ¼”ç¤º DeepEval æ•´åˆ"""
    print("ğŸ§ª RAGFlow DeepEval è©•ä¼°ç³»çµ±")
    print("=" * 50)
    
    # æª¢æŸ¥ OpenAI API å¯†é‘°
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("âš ï¸  æœªè¨­ç½® OPENAI_API_KEY")
        print("   æŸäº›é«˜ç´šè©•ä¼°åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
        print("   ä½ ä»ç„¶å¯ä»¥ä½¿ç”¨åŸºæœ¬çš„è©•ä¼°åŠŸèƒ½")
        print()
    
    # å‰µå»ºè©•ä¼°å™¨
    evaluator = RAGFlowEvaluator(openai_api_key)
    
    # ç²å–æ•¸æ“šé›†
    datasets_result = evaluator.client.list_datasets()
    if not datasets_result['success']:
        print(f"âŒ ç²å–æ•¸æ“šé›†å¤±æ•—: {datasets_result['message']}")
        return
    
    datasets = datasets_result['data']
    if not datasets:
        print("âŒ æ²’æœ‰å¯ç”¨çš„æ•¸æ“šé›†")
        return
    
    print(f"ğŸ“š æ‰¾åˆ° {len(datasets)} å€‹æ•¸æ“šé›†:")
    for i, dataset in enumerate(datasets, 1):
        print(f"  {i}. {dataset.get('name', 'N/A')} (ID: {dataset.get('id', 'N/A')})")
    
    # é¸æ“‡æ•¸æ“šé›†
    try:
        choice = input(f"\nè«‹é¸æ“‡è¦è©•ä¼°çš„æ•¸æ“šé›† [1-{len(datasets)}]: ").strip()
        index = int(choice) - 1
        
        if 0 <= index < len(datasets):
            selected_dataset = datasets[index]
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æ•¸æ“šé›†")
            selected_dataset = datasets[0]
    except (ValueError, KeyboardInterrupt):
        print("âŒ ç„¡æ•ˆè¼¸å…¥ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æ•¸æ“šé›†")
        selected_dataset = datasets[0]
    
    dataset_id = selected_dataset['id']
    dataset_name = selected_dataset['name']
    
    print(f"ğŸ“– é¸æ“‡æ•¸æ“šé›†: {dataset_name}")
    
    # è¨­ç½®èŠå¤©æ©Ÿå™¨äºº
    if not evaluator.setup_chatbot(dataset_id, dataset_name):
        print("âŒ èŠå¤©æ©Ÿå™¨äººè¨­ç½®å¤±æ•—")
        return
    
    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    try:
        num_questions = int(input("è«‹è¼¸å…¥è¦ç”Ÿæˆçš„æ¸¬è©¦å•é¡Œæ•¸é‡ [é è¨­: 5]: ").strip() or "5")
    except ValueError:
        num_questions = 5
    
    test_data = evaluator.generate_test_data_from_documents(dataset_id, num_questions)
    
    if not test_data:
        print("âŒ æ¸¬è©¦æ•¸æ“šç”Ÿæˆå¤±æ•—")
        return
    
    print(f"âœ… æˆåŠŸç”Ÿæˆ {len(test_data)} å€‹æ¸¬è©¦å•é¡Œ")
    
    # é¡¯ç¤ºç”Ÿæˆçš„å•é¡Œ
    print("\nğŸ“ ç”Ÿæˆçš„æ¸¬è©¦å•é¡Œ:")
    for i, data in enumerate(test_data, 1):
        print(f"  {i}. {data['question']}")
    
    # é–‹å§‹è©•ä¼°
    input("\næŒ‰ Enter é–‹å§‹è©•ä¼°...")
    
    results = evaluator.evaluate_test_cases(test_data)
    
    if not results:
        print("âŒ è©•ä¼°å¤±æ•—")
        return
    
    # ç”Ÿæˆå ±å‘Š
    report = evaluator.generate_report(results)
    print(report)
    
    # ä¿å­˜çµæœ
    evaluator.save_results(results)
    
    print("\nğŸ‰ è©•ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main()